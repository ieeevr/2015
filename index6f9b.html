<!DOCTYPE html>
<html lang="en-US">

<!-- Mirrored from www.ieeevr.org/2015/?q=node/49 by HTTrack Website Copier/3.x [XR&CO'2013], Tue, 24 Mar 2015 14:09:22 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta about="/2015/?q=node/49" property="sioc:num_replies" content="0" datatype="xsd:integer" />
<link rel="shortcut icon" href="sites/default/files/icon.png" type="image/png" />
<meta content="Posters" about="/2015/?q=node/49" property="dc:title" />
<link rel="shortlink" href="index6f9b.html?q=node/49" />
<meta name="Generator" content="Drupal 7 (http://drupal.org)" />
<link rel="canonical" href="index6f9b.html?q=node/49" />

<title>Posters | IEEE VR 2015</title>
<style type="text/css" media="all">@import url("modules/system/system.base6d18.css?njikyt");
@import url("modules/system/system.menus6d18.css?njikyt");
@import url("modules/system/system.messages6d18.css?njikyt");
@import url("modules/system/system.theme6d18.css?njikyt");</style>
<style type="text/css" media="all">@import url("modules/comment/comment6d18.css?njikyt");
@import url("modules/field/theme/field6d18.css?njikyt");
@import url("modules/node/node6d18.css?njikyt");
@import url("modules/search/search6d18.css?njikyt");
@import url("modules/user/user6d18.css?njikyt");
@import url("modules/ckeditor/css/ckeditor6d18.css?njikyt");</style>
<style type="text/css" media="all">@import url("themes/business_enterprises/style6d18.css?njikyt");</style>
<script type="text/javascript" src="modules/jquery_update/replace/jquery/1.7/jquery.minc011.js?v=1.7.1"></script>
<script type="text/javascript" src="misc/jquery.once7839.js?v=1.2"></script>
<script type="text/javascript" src="misc/drupal6d18.js?njikyt"></script>
<script type="text/javascript" src="themes/business_enterprises/js/superfish6d18.html?njikyt"></script>
<script type="text/javascript" src="themes/business_enterprises/js/effects6d18.html?njikyt"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
jQuery.extend(Drupal.settings, {"basePath":"\/2015\/","pathPrefix":"","ajaxPageState":{"theme":"business_enterprises","theme_token":"Swq_HPgq5fDHF38kC84fmqqrPcsLsztI_GhnsRoKR10","js":{"modules\/statistics\/statistics.js":1,"modules\/jquery_update\/replace\/jquery\/1.7\/jquery.min.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"themes\/business_enterprises\/js\/superfish.js":1,"themes\/business_enterprises\/js\/effects.js":1},"css":{"modules\/system\/system.base.css":1,"modules\/system\/system.menus.css":1,"modules\/system\/system.messages.css":1,"modules\/system\/system.theme.css":1,"modules\/comment\/comment.css":1,"modules\/field\/theme\/field.css":1,"modules\/node\/node.css":1,"modules\/search\/search.css":1,"modules\/user\/user.css":1,"modules\/ckeditor\/css\/ckeditor.css":1,"themes\/business_enterprises\/style.css":1}},"statistics":{"data":{"nid":"49"},"url":"\/2015\/?q=modules\/statistics\/statistics.php"}});
//--><!]]>
</script>
<!--[if lt IE 9]><script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
</head>
<body class="html not-front not-logged-in one-sidebar sidebar-second page-node page-node- page-node-49 node-type-page">
    <div id="wrapper">
  <div class="inwrap">
    <div id="header-top">
        <div class="logo">
                     <div id="logoimg">
            <a href="index.html" title="Home"><img src="sites/default/files/vr15.png" alt="Home" /></a>
            </div>
                   <div class="sitename">
            <h1><a href="index.html" title="Home"></a></h1>
            <h2></h2>
         </div>
      </div>
	
    <div id="menu-container">
    <div id="main-menu" class="menu-menu-container">
        <ul class="menu"><li class="first leaf"><a href="index.html">Home</a></li>
<li class="expanded active-trail"><a href="index5ffe.html?q=node/6" class="active-trail">Program</a><ul class="menu"><li class="first leaf"><a href="indexf6ef.html?q=node/48">Program</a></li>
<li class="leaf"><a href="index01ce.html?q=node/25">VR Keynote Speaker</a></li>
<li class="leaf"><a href="indexec53.html?q=node/47">Papers</a></li>
<li class="leaf"><a href="index7c87.html?q=node/31">Panels</a></li>
<li class="leaf"><a href="indexb963.html?q=node/39">Workshop Papers</a></li>
<li class="leaf"><a href="indexa7ca.html?q=node/46">Industrial Presentations</a></li>
<li class="leaf"><a href="index4920.html?q=node/45">Lab/Project Presentations</a></li>
<li class="leaf active-trail"><a href="index6f9b.html?q=node/49" title="Attempt to re-order the posters, and regroup them into three sets (Wednesday, Thursday, Friday)" class="active-trail active">Posters</a></li>
<li class="leaf"><a href="index5861.html?q=node/33">Research Demos</a></li>
<li class="leaf"><a href="index6da2.html?q=node/30">Tutorials</a></li>
<li class="last leaf"><a href="indexe737.html?q=node/51">Exhibitors</a></li>
</ul></li>
<li class="expanded"><a href="index763b.html?q=node/5">Call for Participation</a><ul class="menu"><li class="first leaf"><a href="index2a62.html?q=node/3">Long &amp; Short Papers</a></li>
<li class="leaf"><a href="indexd46b.html?q=node/16">Posters</a></li>
<li class="leaf"><a href="indexb0c5.html?q=node/37">Lab/Project Presentations</a></li>
<li class="leaf"><a href="indexca97.html?q=node/36">Industrial Contributions</a></li>
<li class="leaf"><a href="index9f1c.html?q=node/18">Research Demos</a></li>
<li class="leaf"><a href="index0f22.html?q=node/4">Workshops</a></li>
<li class="leaf"><a href="index7c44.html?q=node/14">Tutorials</a></li>
<li class="leaf"><a href="index763f.html?q=node/15">Panels</a></li>
<li class="leaf"><a href="index06d0.html?q=node/20">Videos</a></li>
<li class="leaf"><a href="index28fc.html?q=node/17">Exhibitors and Supporters</a></li>
<li class="leaf"><a href="indexd917.html?q=node/19">Student Volunteers</a></li>
<li class="last leaf"><a href="index89f8.html?q=node/40">Doctoral Consortium</a></li>
</ul></li>
<li class="expanded"><a href="index72dc.html?q=node/23">Participate</a><ul class="menu"><li class="first leaf"><a href="index8860.html?q=node/7">Venue and transportation</a></li>
<li class="leaf"><a href="index71da.html?q=node/24" title="added Saintes-Maries-de-la-Mer and Aigues-Mortes in the &quot;must-see&quot; section">Arles-Camargue-Provence</a></li>
<li class="leaf"><a href="index77ab.html?q=node/22">Registration</a></li>
<li class="leaf"><a href="indexf7a0.html?q=node/41">Accomodations</a></li>
<li class="leaf"><a href="index3f6e.html?q=node/43">Visa FAQ</a></li>
<li class="last leaf"><a href="index5854.html?q=node/52">Presenter Instructions</a></li>
</ul></li>
<li class="expanded"><a href="index63ba.html?q=node/8">Committees</a><ul class="menu"><li class="first leaf"><a href="indexf02c.html?q=node/11">Organization Committee</a></li>
<li class="leaf"><a href="indexd022.html?q=node/12">Program Committee</a></li>
<li class="last leaf"><a href="index67db.html?q=node/13">Steering Committee</a></li>
</ul></li>
<li class="last leaf"><a href="http://3dui.org/" title="">IEEE 3DUI 2015</a></li>
</ul>      </div>
    </div>
    </div>
  <div id="content-container">

    
 <div id="page-container">
  
  
  <div id="content">
  <div class="breadcrumb"><h2 class="element-invisible">You are here</h2><nav class="breadcrumb"><a href="index.html">Home</a> » <a href="index5ffe.html?q=node/6">Program</a> » Posters</nav></div>  <section id="main" role="main" class="post">
        <a id="main-content"></a>
            <div class="title"><h2 class="title" id="page-title">Posters</h2></div>                <div class="region region-content">
  <div id="block-system-main" class="block block-system">

      
  <div class="content">
                            
      
    
  <div class="content">
    <div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item even" property="content:encoded"><p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="PostersWed" name="PostersWed"></a></span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">The following posters have been accepted for publication at IEEE VR 2015:</span></span></p>
<div> </div>
<div><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Posters presented on Wednesday</strong></span></span></div>
<ul><li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#115">Wednesday 1 - Investigating Visual Dominance with a Virtual Driving Task</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#153">Wednesday 2 - VR and AR Simulator for Neurosurgical Training</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#242">Wednesday 3 - High-Definition Digital Display Case with the Image-based Interaction</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#112">Wednesday 4 - Volumetric Calibration and Registration of RGBD-Sensors</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#155">Wednesday 5 - Touching sounds : Perception of the Curvature of Auditory Virtual Surfaces</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#162">Wednesday 6 - Wayfinding by Auditory Cues in Virtual Environments</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#177">Wednesday 7 - Portable-Spheree: A portable 3D Perspective-Corrected Interactive Spherical Scalable Display</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#132">Wednesday 8 - A Multi-Layer Approach of Interactive Path Planning for Assisted Manipulation in Virtual Reality</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#202">Wednesday 9 - Visual-Olfactory Immersive Environment For Product Evaluation</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#326">Wednesday 10 - Comparative Evaluation of Stylized versus Realistic Representation of Virtual Humans on Empathetic Responses in Simulated Interpersonal Experiences</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#165">Wednesday 11 - Validation of SplitVector Encoding and Stereoscopy for Visualizing Quantum Physics Data in Virtual Environments</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#147">Wednesday 12 - A Building-Wide Indoor Tracking System for Augmented Reality</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#138">Wednesday 13 - Collaborative Table-Top VR Display for Neurosurgical Planning</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#257">Wednesday 14 - Investigating the Impact of Perturbed Visual and Proprioceptive information in Near-Field Immersive Virtual Environment</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#127">Wednesday 15 - Using Augmented Reality to Support Situated Analytics</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#143">Wednesday 16 - Vision-based Multi-Person Foot Tracking for CAVE Systems with Under-Floor Projection</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#144">Wednesday 17 - Effects and Applicability of Rotation Gain in CAVE-like Environments</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#159">Wednesday 18 - flapAssist: How the integration of VR and Visualization Tools fosters the factory planning process</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#157">Wednesday 19 - An Immersive Labyrinth</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#160">Wednesday 20 - Towards Context-Sensitive Reorientation for Real Walking in Virtual Reality</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#137">Wednesday 21 - Incorporating D3.js Information Visualization into Immersive Virtual Environments</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#122">Wednesday 22 - Using Interactive Virtual Characters in Social Neuroscience</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#279">Wednesday 23 - Towards A High Fidelity Simulation of the Kidney Biopsy Procedure</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#145">Wednesday 24 - AR-SSVEP for Brain-Machine Interface: Estimating User's Gaze in Head-mounted Display with USB camera</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#180">Wednesday 25 - Five Senses Theatre Project: Sharing Experiences through Bodily Ultra-Reality</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#118">Wednesday 26 - Robust Enhancement of Depth Images from Kinect Sensor</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#176" id="PostersThu" name="PostersThu">Wednesday 27 - Desktop Versions of the String-based Haptic Interface- SPIDAR</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#225">Wednesday 28 - Registration and Projection method of tumor region projection for breast cancer surgery assistance</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#170">Wednesday 29 - 3D Node Localization from Node-to-Node Distance Information using Cross-Entropy Method</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#178">Wednesday 30 - BlenderVR: Open-source framework for interactive and immersive VR</a></span></span></li>
</ul><p> </p>
<div><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Posters presented on Thursday</strong></span></span></div>
<ul><li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#110">Thursday 1 - Scalable Metadata </a><a href="#110">In- and Output for Multi-platform Data Annotation Applications</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#163">Thursday 2 - An Experimental Study on the Virtual Representation of Children</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#131">Thursday 3 - Human-Avatar Interaction and Recognition Memory according to Interaction Types and Methods</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#134">Thursday 4 - Dynamic Hierarchical Virtual Button-based Hand Interaction for Wearable AR</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#203">Thursday 5 - 3D Position Measurement of Planar Photo Detector Using Gradient Patterns</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#138">Thursday 6 - What Can We Feel on the Back of the Tablet? -- A Thin Mechanism to Display Two Dimensional Motion on the Back and Its Characteristics –</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#156">Thursday 7 - Cooperation in Virtual Environments with Individual Views</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#133">Thursday 8 - Interaction with Virtual Agents – Comparison of the participants’ experience between an IVR and a semi-IVR system</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#129">Thursday 9 - Multiple Devices as Windows for Virtual Environment</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#161">Thursday 10 - Synthesis of Omnidirectional Movie using a Set of Key Frame Panoramic Images</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#102">Thursday 11 - The Influence of Virtual Reality and Mixed Reality Environments com-bined with two different Navigation Methods on Presence</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#124">Thusday 12 - Avatar Embodiment Realism and Virtual Fitness Training</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#294">Thursday 13 - Avatar Anthropomorphism and Illusion of Body Ownership in VR</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#125">Thursday 14 - Influence of Avatar Realism on Stressful Situation in VR</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#119">Thursday 15 - Extending Touch-less Interaction on Vision Based Wearable Device</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#303">Thursday 16 - Blind in a Virtual World: Using Sensory Substitution for Generically Increasing the Accesibilty of Graphical Virtual Environments</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#230">Thursday 17 - I Built It! - Exploring the effects of Customizable Virtual Humans on Adolescents with ASD</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#304">Thursday 18 - The Effects of Olfaction on Training Transfer for an Assembly Task</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#104">Thursday 19 - MRI Overlay System Using Optical See-Through for Marking Assistance</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#135">Thursday 20 - Continuous Automatic Calibration for Optical See-Through Displays</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#190">Thursday 21 - Comparing the Performance of Natural, Semi-Natural, and Non-Natural Locomotion Techniques in Virtual Reality</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#169">Thursday 22 - Implementation of on-site virtual time machine for mobile devices</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#105">Thursday 23 - The Effect of Head Mounted Display Weight and Locomotion Method on the Perceived Naturalness of Virtual Walking Speeds</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#150">Thursday 24 - Third person's footsteps enhanced walking sensation of seated person</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#151">Thursday 25 - Does Vibrotactile Intercommunication Increase Collaboration?</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#113">Thursday 26 - Coupled-Clay: Physical-Virtual 3D Collaborative Interaction Environment</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#204">Thursday 27 - GPU-accelerated Attention Map Generation for Dynamic 3D Scenes</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#123" id="PostersFri" name="PostersFri">Thursday 28 - A Procedure for Accurate Calibration of a Tebletop Haploscope AR Environment</a></span></span></li>
</ul><p> </p>
<div><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Posters presented on Friday</strong></span></span></div>
<ul><li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#140">Friday 1 - Using Astigmatism in Wide Angle HMDs to Improve Rendering</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#109">Friday 2 - Shark Punch: A Virtual Reality Game for Aquatic Rehabilitation</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#157">Friday 3 - Real-time SLAM for static multi-objects learning and tracking applied to augmented reality applications</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#109">Friday 4 - Social Presence with Virtual Glass</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#297">Friday 5 - Semi-automatic Calibration of a Projector-Camera System Using Arbitrary Objects With Known Geometry</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#175">Friday 6 - Navigation in REVERIE's Virtual Environments</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#141">Friday 7 - Collaborative Telepresence Workspaces for Space Operation and Science</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#130">Friday 8 - Does Virtual Reality really affect visual perception of egocentric distance?</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#288">Friday 9 - A GPU-Based Adaptive Algorithm for Non-Rigid Surface Registration</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#146">Friday 10 - Characteristics of virtual walking sensation created by a 3-dof motion seat</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#214">Friday 11 - Self-Characterstics and Sound in Immersive Virtual Reality - Estimating Avatar Weight from Footstep Sounds</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#213">Friday 12 - Wings and Flying in Immersive VR - Controller Type, Sound Effects and Experienced Ownership and Agency</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#158">Friday 13 - Optical See-through HUDs Effect on Depth Judgments of Real World Objects</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#117">Friday 14 - EVE: Exercise in Virtual Environments</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#139">Friday 15 - Subjective Evaluation of Peripheral Viewing during Exposure to a 2D/3D Video Clip</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#161">Friday 16 - Zoom Factor Compensation for Monocular SLAM</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#308">Firday 17 - A Modified Tactile Brush Algorithm for Complex Touch Gestures</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#265">Friday 18 - Experiencing Interior Environments: New Approaches for the Immersive Display of Large-Scale Pointcloud Data</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#164">Friday 19 - Landscape Change From Daytime To Nighttime Under Augmented Reality Environment</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#152">Friday 20 - Impact of Illusory Resistance on Finger Walking Behavior</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#154">Firday 21 - Development of a Wearable Haptic Device with Pneumatic Artificial Muscles and MR brake</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#272">Friday 22 - Preliminary Evaluation of a Virtual Needle Insertion Training System</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#114">Friday 23 - From visual cues to climate perception in virtual urban environments</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#218">Friday 24 - HorizontalDragger: a Freehand Remote Selector for Object Acquisition</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#136">Friday 25 - A Real-Time Welding Training System Based on Virtual Reality</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#234">Friday 26 - Transparent Cockpit Using Telexistence</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#171">Friday 27 - Flying Robot Manipulation System Using a Virtual Plane</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#182">Friday 28 - Binocular Interface: Interaction Techniques Considering Binocular Parallax for a Large Display</a></span></span></li>
<li><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a href="#136">Friday 29 - Tracking Human Locomotion by Relative Positional Feet Tracking</a></span></span></li>
</ul><p> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="115" name="115"></a></span></span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Wednesday 1</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Investigating Visual Dominance with a Virtual Driving Task</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abdulaziz Alshaer - University of Otago's Information Science Department              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Holger Regenbrecht - University of Otago's Information Science Department              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">David O'Hare - University of Otago's Psychology Department </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author:  Holger Regenbrecht</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Most interactive input devices for virtual reality-based simulators are proprietary and expensive. Can they be substituted with standard, inexpensive devices if the virtual representation of the input device looks and acts like the original? Visual dominance theory, where the visual aspects of the displayed input device within the virtual environment should override the haptic aspects of the real device, would appear to support such a possibility. We tested this visual dominance theory in a VR power wheelchair simulator scenario comparing standard gaming and proprietary wheelchair joysticks in combinations with their virtual counterparts and measured the effects on driving performance.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="153" name="153"></a></span></span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Wednesday 2</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>VR and AR Simulator for Neurosurgical Training</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Ryan Armstrong - University of Western Ontario </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sandrine de Ribaupierre - University of Western Ontario </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Dayna Noltie - University of Western Ontario </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Matt Kramers - University of Western Ontario </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Roy Eagleson - University of Western Ontario </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Roy Eagleson</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: The placement of an external ventricular drain is one of the most commonly performed neurosurgical procedures, and consequently, is an essential skill to be mastered by neurosurgical trainees. The drain placement involves analyzing images from the patient, choosing an entry point and deciding on a trajectory to hit the ventricle. In this paper, we describe the development of a simulation environment to train residents, coupled with an AR image-guidance tool. Performance is evaluated using Fitts’ methodology (Fitts, 1954), which respects the users ability to trade-off speed and accuracy.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="242" name="242"></a></span></span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Wednesday 3</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>High-Definition Digital Display Case with the Image-based Interaction</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yuki Ban, The University of Tokyo</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Takashi Kajinami, The University of Tokyo</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Takuji Narumi, The University of Tokyo</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tomohiro Tanikawa, The University of Tokyo</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Michitaka Hirose, The University of Tokyo</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Yuki Ban</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: This paper proposes a high-definition digital display case for manipulating a virtual exhibit that has linking mechanisms. This technique enhances the understanding of dynamic exhibits. It is difficult to construct interactive contents of dynamic virtual exhibits, because measuring the mechanism invokes the risk of an exhibit's deterioration, and it takes tremendous efforts to create a fine spun computer graphics (CG) model for mechanisms. Therefore, we propose an image-based interaction method that uses image-based rendering to construct interactive contents for dynamic virtual exhibits using the interpolation between exhibit pictures with a number of deformational conditions and viewpoints. Using this method, we construct a high-definition digital showcase and exhibit the interactive content at a museum to evaluate the availability of our system.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="112" name="112"></a></span></span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Wednesday 4</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Volumetric Calibration and Registration of RGBD-Sensors</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Stephan Beck, Bauhaus-Universität Weimar</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Bernd Froehlich, Bauhaus-Universität Weimar</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Stephan Beck</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We present an integrated approach for the calibration and registration of color and depth (RGBD) sensors into a joint coordinate system without explicitly identifying intrinsic or extrinsic camera parameters. Our method employs a tracked checkerboard to establish a number of correspondences between positions in color and depth camera space and in world space. These correspondences are used to construct a single calibration and registration volume per RGBD sensor which directly maps raw depth sensor values into a joint coordinate system and to their associated color values. Our evaluation demonstrates an accuracy with an average 3D error below 3 mm and an average texture deviation smaller than 0.5 pixels for a space of about 1.5 m x 1.8 m x 1.5 m.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="155" name="155"></a></span></span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Wednesday 5</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Touching sounds : Perception of the Curvature of Auditory Virtual Surfaces</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Eric O. Boyer, LPP AVOC Team, UMR 8242 CNRS-Université Paris Descartes, Paris</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Lucyle Vandevoorde, UFR STAPS - Université Paris Descartes</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Frédéric Bevilacqua, STMS, IRCAM-CNRS-UPMC, Paris</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sylvain Hanneton, LPP AVOC Team, UMR 8242 CNRS-Université Paris Descartes, Paris</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Eric Boyer</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: In this study, we investigated the ability of blindfolded adults to discriminate between concave and convex auditory virtual surfaces. We used a Leap Motion device to measure the movements of the hand and fingers. Participants were asked to explore the space above the device with the palm of one hand and an auditory feedback was produced only when the palm was moving into the boundaries of the surface. In order to demonstrate that curvature direction was correctly perceived by our participants, we estimated their discrimination thresholds with a psychophysical staircase procedure. Two groups of participants were fed with two different sonification of the surface. Results showed that most of the participants were able to learn the task. The best results were obtained with an auditory feedback related to the component of the hand velocity tangential to the virtual surface. This work proposes a contribution to the introduction in virtual reality of auditory virtual objects.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="162" name="162"></a>Wednesday 6</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Wayfinding by Auditory Cues in Virtual Environments</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Ayana Burkins - Department of Computer Science         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Regis Kopper - Duke immersive Virtual Environment   </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Ayana Burkins</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Wayfinding is a typical task in virtual environments. Real-world aids such as maps and Global Position Systems can present unique challenges due to the potential for cognitive overload and the immersive nature of the environment. This work presents the results of a pilot study involving the use of auditory cues as a wayfinding aid in a virtual mall environment. The data suggests that users are able to complete wayfinding tasks faster and more accurately in an environment containing sound cues than in one without.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="177" name="177"></a>Wednesday 7</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Portable-Spheree: A portable 3D Perspective-Corrected Interactive Spherical Scalable Display</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Marcio Cabral - Polytechnic School - University of Sao Paulo     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Fernando Ferreira - Federal University of ABC          </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Olavo Belloc - Polytechnic School - University of Sao Paulo     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Celso Kurashima - Federal University of ABC          </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Roseli Lopes - Polytechnic School - University of Sao Paulo     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Ian Stavness - University of Saskatchewan      </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Junia Anacleto - Federal University of Sao Carlos             </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sidney Fels - University of British Columbia  </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Marcelo Zuffo - Polytechnic School - University of Sao Paulo     </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Marcio Cabral</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: In this poster we present Portable-Spheree, an interactive spherical rear-projected 3D-content-display that provides perspective-corrected views according to the user's head position, to provide parallax, shading and occlusion depth cues. Portable-Spheree is an evolution of the Spheree and it is developed in a smaller form factor, using more projectors and a dark-translucent screen with increased contrast. We present some preliminary results of this new configuration as well as applications with spatial interaction that might benefit from this new form factor.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">_________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="132" name="132"></a>Wednesday 8</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>A Multi-Layer Approach of Interactive Path Planning for Assisted Manipulation in Virtual Reality</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Simon Cailhol, ENIT-LGP</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Philippe Fillatreau, ENIT-LGP</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jean-Yves Fourquet, ENIT-LGP</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yingshen Zhao, ENIT-LGP</span></span></p>
<p class="rtecenter"> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Jean-Yves Fourquet</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract:This work considers VR applications dealing with objects manipulation (such as industrial product assembly, disassembly or maintenance simulation). For such applications, the operator performing the simulation can be assisted by path planning techniques from the robotics research field. A novel automatic path planner involving geometrical, topological and semantic information of the environment is proposed for the guidance of the user through a haptic device. The interaction allows on one hand, the automatic path planner to provide assistance to the human operator, and on the other hand, the operator to reset the whole planning process suggesting a better suited path. Control sharing techniques are used to improve the assisted manipulation ergonomics by dynamically balancing the automatic path planner authority according to the operator involvement in the task, and by predicting user’s intent to integrate it as early as possible in the planning process.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="202" name="202"></a>Wednesday 9</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Visual-Olfactory Immersive Environment For Product Evaluation</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Marina Carulli, Politecnico di Milano</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Monica Bordegoni, Politecnico di Milano</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Umberto Cugini, Politecnico di Milano</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Monica Bordegoni</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: The sense of smell has a great importance in our daily life. In recent years, smells have been used for marketing purposes with the aim of improving the person's mood and of communicating information about products as household cleaners and food. However, the scent design discipline can be also applied to any kind of products to communicate their features to customers. In the area of Virtual Reality several researches have focused on integrating smells in virtual environments. The research questions addressed in this work concern whether Virtual Prototypes, including the sense of smell, can be used for evaluating products as effectively as studies performed in real environments, and also whether smells can contribute to increase the users' sense of presence in the virtual environment. For this purpose, a Virtual Reality experimental framework including a prototype of a wearable olfactory display has been set up, and experimental tests have been performed.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="326" name="326"></a>Wednesday 10</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Comparative Evaluation of Stylized versus Realistic Representation of Virtual Humans on Empathetic Responses in Simulated Interpersonal Experiences</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Himanshu Chaturvedi, Clemson University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Nathan Newsome, Clemson University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sabarish Babu, Clemson University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">June Luo, Clemson University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tania Roy, Clemson University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Shaundra Daily, Clemson University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jeffrey Bertrand, Clemson University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tracy Fasolino, Clemson University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Elham Ebrahimi, Clemson University</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Himanshu Chaturvedi</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: The effectiveness of visual realism of virtual characters in engaging users and eliciting affective responses has been an open question. We empirically evaluated the effects of realistic vs. non-realistic rendering of virtual humans on the emotional response of participants in a medical virtual reality system that was designed to educate users to recognize the signs and symptoms of patient deterioration. In a between-subjects experiment protocol, participants interacted with one of three different appearances of a virtual patient, namely realistic, non-realistic cartoon-shaded and charcoal-sketch like conditions. Emotional impact of the rendering conditions was measured via a combination of subjective and objective metrics.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="165" name="165"></a>Wednesday 11</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Validation of SplitVector Encoding and Stereoscopy for Visualizing Quantum Physics Data in Virtual Environments</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jian Chen - University of Maryland, Baltimore County         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Henan Zhao - University of Maryland, Baltimore County         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Wesley Griffin - University of Maryland, Baltimore County         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Judith E. Terrill - National Institute of Standard and Technology (NIST)  </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Garnett W. Bryant - National Institute of Standard and Technology (NIST)  </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Jian Chen</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We designed and evaluated SplitVector, a new vector field display approach to help scientists perform new discrimination tasks on scientific data shown in virtual environments (VEs). We present an empirical study to compare the SplitVector approach with three other approaches in information-rich VEs. Twenty participants performed three domain analyses tasks. Our empirical study results suggest the following: (1) SplitVectors improve the accuracy by about 10 times compared to the linear mapping and by 4 times to log in discrimination tasks and (2) SplitVectors lead to no significant differences from the IRVE text display approach, yet reduce the clutter.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="147" name="147"></a>Wednesday 12</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>A Building-Wide Indoor Tracking System for Augmented Reality</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Stéphane Côté - Bentley Systems            </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">François Rheault - Sherbrooke University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Julien Barnard - Université Laval              </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Stéphane Côté</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Buildings require regular maintenance, and augmented reality (AR) could advantageously be used to facilitate the process. However, such AR systems would require accurate tracking to meet the needs of engineers, and work accurately in entire buildings. In this project, we propose a hybrid system combining low accuracy radio-based tracking, and high accuracy tracking using depth images obtained from range cameras. Results show tracking accuracy that would be compatible with AR applications and that would be constant within a building.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="138" name="138"></a>Wednesday 13</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Collaborative Table-Top VR Display for Neurosurgical Planning</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Roy Eagleson, Western Ontario, London, Ontario, Canada</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Patrick Wucherer, Technische Universität München, Germany</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Philipp Stefan, Technische Universität München, Germany</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yaroslav Duschko, Technische Universität München,Germany</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sandrine de Ribaupierre, Robarts Research Institute, Western University, Canada</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Christian Vollmar, Klinikum der  Universität München, Germany</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Pascal Fallavollita, Technische Universität München,Germany</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Nassir Navab, Technische Universität München,Germany</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Roy Eagleson</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We present a prototype of a system in development for pre-operative planning. The proposed NeuroTable uses a combination of traditional rendering and novel visualization techniques rendered to facilitate real-time collaboration between neurosurgeons during intervention planning. A set of multimodal 2D and 3D renderings convey the relation between the region of interest and the surrounding anatomical structures. A haptic device is used for interaction with the NeuroTable to facilitate immersive control over the 3D cursor and navigation modes for the neurosurgeons during their discourse of planning. A pilot experimental study was conducted to assess the performance of users in targeting points within the preoperative 3D scan. Then, two clinicians participated in the evaluation of the table in discussing and planning a case. Results indicate that the NeuroTable facilitated the discourse and we discuss the results of the speed and accuracy for the specification of entry and target points.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="257" name="257"></a>Wednesday 14</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Investigating the Impact of Perturbed Visual and Proprioceptive information in Near-Field Immersive Virtual Environment</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Elham Ebrahimi, Clemson University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Bliss M. Altenhoff, Clemson University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Christopher C. Pagano, Clemson University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sabarish V. Babu, Clemson University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">J. Adam Jones, Clemson University</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Elham Ebrahimi</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We report the results of an empirical evaluation to examine the carryover effects of calibrations to one of three perturbations of visual and proprioceptive feedback: i) Minus condition (-20% gain) in which a visual stylus appeared at 80% of the distance of a physical stylus, ii) Neutral condition (0% gain) in which a visual stylus was co-located with a physical stylus, and iii) Plus condition (+20% gain) in which the visual stylus appeared at 120% of the distance of the physical stylus. Feedback was shown to calibrate distance judgments quickly within an IVE, with estimates being farthest after calibrating to visual information appearing nearer (Minus condition), and nearest after calibrating to visual information appearing further (Plus condition). </span></span></p>
<p> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="127" name="127"></a>Wednesday 15</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Using Augmented Reality to Support Situated Analytics</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Neven ElSayed - University of South Australia    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Bruce Thomas - University of South Australia    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Ross Smith - University of South Australia    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Kim Marriott - Monash University        </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Julia Piantadosi - University of South Australia    </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Neven A. M. ElSayed</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We draw from the domains of Visual Analytics and Augmented Reality to support a new form of in-situ interactive visual analysis. We present a Situated Analytics model, a novel interaction, and a visualization concept for reasoning support. Situated Analytics has four primary elements: situated information, abstract information, augmented reality interaction, and analytical interaction.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="143" name="143"></a>Wednesday 16</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Vision-based Multi-Person Foot Tracking for CAVE Systems with Under-Floor Projection</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sebastian Freitag - Virtual Reality Group, RWTH Aachen University              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sebastian Schmitz - RWTH Aachen University           </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Torsten W. Kuhlen - Virtual Reality Group, RWTH Aachen University              </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Sebastian Freitag</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: In this work, we present an approach for tracking the feet of multiple users in CAVE-like systems with under-floor projection. It is based on low-cost consumer cameras, does not require users to wear additional equipment, and can be installed without modifying existing components. If the brightness of the floor projection does not contain too much variation, the feet of several people can be reliably tracked and assigned to individuals.</span></span></p>
<p> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="144" name="144"></a>Wednesday 17</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Effects and Applicability of Rotation Gain in CAVE-like Environments</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sebastian Freitag - Virtual Reality Group, RWTH Aachen University              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Benjamin Weyers - Virtual Reality Group, RWTH Aachen University              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Torsten W. Kuhlen - Virtual Reality Group, RWTH Aachen University              </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Sebastian Freitag</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: In this work, we report on a pilot study we conducted, and on a study design, to examine the effects and applicability of rotation gain in CAVE-like virtual environments. The results of the study will give recommendations for the maximum levels of rotation gain that are reasonable in algorithms for enlarging the virtual field of regard or redirected walking.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="159" name="159"></a>Wednesday 18</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>flapAssist: How the integration of VR and Visualization Tools fosters the factory planning process</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sascha Gebhardt - RWTH Aachen University           </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sebastian Pick - RWTH Aachen University           </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Hanno Voet - RWTH Aachen University           </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Julian Utsch - RWTH Aachen University           </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Toufik al Khawli - Fraunhofer ILT </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Urs Eppelt - Fraunhofer ILT </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Rudolf Reinhard - RWTH Aachen University           </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Christian Büscher - RWTH Aachen University           </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Bernd Hentschel - RWTH Aachen University           </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Torsten W. Kuhlen - Jülich Supercomputing Centre </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Sascha Gebhardt</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Virtual Reality (VR) systems are of growing importance to aid decision support in factory planning. While current solutions either focus on virtual walkthroughs or the visualization of more abstract information, a solution that provides both, does currently not exist. To close this gap, we present a holistic VR application, called flapAssist. It is meant to serve as a platform for planning the layout of factories, while also providing a wide range of analysis features. By being scalable from desktops to CAVEs and providing a link to a central integration platform, flapAssist integrates well in established factory planning workflows.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="157" name="157"></a>Wednesday 19</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>An Immersive Labyrinth</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Copper Giloth - University of Massachusetts    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jonathan Tanant - JonLab </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Copper Frances Giloth</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We have developed a 3D VR digital heritage App providing a virtual experience of an elaborate labyrinth that existed in the gardens of the Chateau de Versailles in the 17th and 18th centuries. We are now taking this App into an immersive environment (using an Oculus Rift headset); we will report on the progress of and conclusions from this porting process.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="160" name="160"></a>Wednesday 20</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Towards Context-Sensitive Reorientation for Real Walking in Virtual Reality</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Timofey Grechkin - USC Institute for Creative Technologies              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Mahdi Azmandian - USC Institute for Creative Technologies              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Mark Bolas - USC Institute for Creative Technologies              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Evan Suma - USC Institute for Creative Technologies              </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author:</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Redirected walking techniques help overcome physical limitations for natural locomotion in virtual reality. Though subtle perceptual manipulations are helpful, it is inevitable that users will approach critical boundary limits. Current solutions to this problem involve breaks in presence by introducing distractors, or freezing the virtual world relative to the user’s perspective. We propose an approach that integrates into the virtual world narrative to draw users’ attention and cause them to temporarily alter their course to avoid going off bounds. This method ties together unnoticeable translation, rotation, and curvature gains, efficiently reorienting the user while maintaining the user’s sense of immersion.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="137" name="137"></a>Wednesday 21</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Incorporating D3.js Information Visualization into Immersive Virtual Environments</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Wesley Griffin - National Institute of Standards and Technology              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Danny Catacora - National Institute of Standards and Technology              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Steven Satterfield - National Institute of Standards and Technology              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jeffrey Bullard - National Institute of Standards and Technology              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Judith Terrill - National Institute of Standards and Technology              </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Wesley Griffin</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We have created an integrated interactive visualization and analysis environment that can be used immersively or on the desktop to study a simulation of microstructure development during hydration or degradation of cement pastes and concrete. Our environment combines traditional 3D scientific data visualization with 2D information visualization using D3.js running in a web browser. By incorporating D3.js, our visualization allowed the scientist to quickly diagnose and debug errors in the parallel implementation of the simulation.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="122" name="122"></a>Wednesday 22</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Using Interactive Virtual Characters in Social Neuroscience</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Joanna Hale - University College London         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Xueni Pan - University College London         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Antonia F. de C. Hamilton - University College London         </span></span></p>
<p> </p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Xueni Pan</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Recent advances in the technical ability to build realistic and interactive Virtual Environments have allowed neuroscientists to study social cognition and behavior in virtual reality. This is particularly useful in the study of social neuroscience, where the physical appearance and motion of Virtual Characters can be fully controlled. In this work, we present the design, implementation, and preliminary results of two case studies exploring different types of social cognition (congruency effect and mimicry) using interactive Virtual Characters animated with either real-time streamed or pre-recorded motion captured data.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="279" name="279"></a>Wednesday 23</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Towards A High Fidelity Simulation of the Kidney Biopsy Procedure</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Gareth Henshall, Bangor University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Serban Pop, Bangor University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Marc Edwards, Bangor University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Llyr ap Cenydd, Bangor University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Nigel John, Bangor University</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Gareth Henshall</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Work in progress for the development of a novel virtual training environment for training a kidney biopsy procedure is presented. Our goal is to provide an affordable high fidelity simulation through the integration of some of the latest off-the-shelf technology components. The range of forces that are encountered during this procedure have been recorded using a custom designed force sensitive glove and then applied within the simulation.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="145" name="145"></a>Wdnesday 24</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>AR-SSVEP for Brain-Machine Interface: Estimating User's Gaze in Head-mounted Display with USB camera</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Shuto Horii - Toyohashi University of Technology     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Shigeki Nakauchi - Toyohashi University of Technology     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Michiteru Kitazaki - Toyohashi University of Technology     </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Shuto Horii</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We aim to develop a brain-machine interface (BMI) system that estimates user's gaze or attention on an object to pick it up in the real world with augmented reality technology. We measured steady-state visual evoked potential (SSVEP) using luminance and/or contrast modulated flickers of photographic scenes presented on a head-mounted display (HMD), and then measured SSVEP using luminance and contrast modulated flickers at AR-markers in real scenes that were online captured by a USB camera and presented on the HMD. We obtained significantly good performance for future application to online estimation of gaze.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="180" name="180"></a>Wednesday 25</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Five Senses Theatre Project: Sharing Experiences through Bodily Ultra-Reality</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yasushi Ikei - Tokyo Metro Univ         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Seiya Shimabukuro - Tokyo Metro Univ         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Shunki Kato - Tokyo Metro Univ         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Kohei Komase - Tokyo Metro Univ         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yujiro Okuya - Tokyo Metro Univ         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Koichi Hirota - U-Tokyo             </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Michiteru Kitazaki - Toyohashi University of Technology     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tomohiro Amemiya - NTT       </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Yasushi Ikei</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: The Five Senses Theatre project was established for the development of a basic technology that enables the user to relive a spatial motion of other persons as if the user him/her-self experienced it in person. This technology aims to duplicate a bodily experience performed in the real space and to pass it to the other person. The system creates the sensation of a self-body motion by multisensory stimulation, specifically vestibular and proprioception, provided to the real body passively. A walking for a sightseeing and a legend run of a top athlete were the first examples of spatial experience copy.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="118" name="118"></a>Wednesday 26</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Robust Enhancement of Depth Images from Kinect Sensor</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">ABM Tariqul Islam - PhD Student, Visual Computing Lab, University of Rostock, Germany  </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Christian Scheel - PhD Student, Visual Computing Lab, University of Rostock, Germany  </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Renato Pajarola - Professor, Visualization and MultiMedia Lab, University of Zürich, Switzerland               </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Oliver Staadt - Professor, Visual Computing Lab, University of Rostock, Germany        </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: ABM Tariqul Islam</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We propose a new method to fill missing/invalid values in depth images generated from the Kinect depth sensor. To fill the missing depth values, we use a robust least median of squares (LMedS) approach. We apply our method for telepresence environments, where Kinects are used very often for reconstructing the captured scene in 3D. We introduce a modified 1D LMedS approach for efficient traversal of consecutive image frames. Our approach solves the unstable nature of depth values in static scenes that is perceived as flickering. We obtain very good result both for static and moving objects inside a scene.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="176" name="176"></a>Wednesday 27</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Desktop Versions of the String-based Haptic Interface- SPIDAR</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Anusha Jayasiri - Tokyo Institute of technology  </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Shuhan Ma - Tokyo Institute of technology  </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yihan Qian - Tokyo Institute of technology  </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Katsuhito Akahane - Tokyo Institute of technology  </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Makoto Sato - Tokyo Institute of technology  </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Shuhan Ma</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: There is a vast development and significant involvement of haptic interfaces in the world for virtual reality applications. SPIDAR, which stands for `SPace Interface Device for Artificial Reality`, is a string-based, friendly human interface on the Sato Makoto Laboratory in the Tokyo Institute of Technology can be used in various types of virtual reality applications for simple pick and place tasks to more complicated physical interactions in virtual worlds. Among the family of SPIDAR devices, here we introduce the research and development of some desktop versions of SPIDAR haptic interfaces called SPIDAR-G, SPIDAR-I and SPIDAR-mouse.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="225" name="225"></a>Wednesday 28</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Registeration and Projection method of tumor region projection for breast cancer surgery assistance</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Motoko Kanegae, Graduate School of Science and Technology, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jun Morita, Graduate School of Science and Technology, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sho Shimamura, Graduate School of Science and Technology, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yuji Uema, Graduate School of Media Design, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Maiko Takahashi, Department of Surgery, School of Medicine, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Masahiko Inami, Graduate School of Media Design, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tetsu Hayashida, Department of Surgery, School of Medicine, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Maki Sugimoto, Graduate School of Science and Technology, Keio University</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Jun Morita</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: This paper introduces a registration and projection method for directly projecting the tumor region for breast cancer surgery assistance based on the breast procedure of our collaborating doctor. We investigated the steps of the breast cancer procedure of our collaborating doctor and how it can be applied for tumor region projection. We propose a novel way of MRI acquisition so we may correlate the MRI coordinates to the patient in the real world. By calculating the transformation matrix from the MRI coordinates and the coordinates from the markers that is on the patient, we are able to register the acquired MRI data to the patient. Our registration and presentation method of the tumor region was then evaluated by medical doctors.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="170" name="170"></a>Wednesday 29</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>3D Node Localization from Node-to-Node Distance Information using Cross-Entropy Method</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Shohei Ukawa - Osaka Univ.      </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tatsuya Shinada - Osaka Univ.      </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Masanori Hashimoto - Osaka Univ.      </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yuichi Itoh - Osaka Univ.      </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Takao Onoye - Osaka Univ.      </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Shohei Ukawa</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: This paper proposes a 3D node localization method that uses cross-entropy method for the 3D modeling system.The proposed localization method statistically estimates the most probable positions overcoming measurement errors through iterative sample generation and evaluation. The generated samples are evaluated in parallel, and then a significant speedup can be obtained. We also demonstrate that the iterative sample generation and evaluation performed in parallel are highly compatible with interactive node movement.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="178" name="178"></a>Wednesday 1</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>BlenderVR: Open-source framework for interactive and immersive VR</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Brian F.G. Katz - LIMSI-CNRS      </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Dalai Q. Felinto - LIMSI-CNRS      </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Damien Touraine - LIMSI-CNRS      </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">David Poirier-Quinot - LIMSI-CNRS      </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Patrick Bourdot - LIMSI-CNRS      </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Dalai Felinto</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: BlenderVR is an open-source framework for interactive/immersive applications based on extending the Blender Game Engine to Virtual Reality. BlenderVR (a generalization of BlenderCAVE) now addresses additional platforms (e.g., HMD, video-walls). BlenderVR provides a flexible easy to use framework for the creation of VR applications for various platforms, employing the power of the BGE graphics rendering and physics engines. Compatible with 3 major Operating Systems, BlenderVR is developed by VR researchers with support from the Blender Community. BlenderVR currently handles multi-screen/multi-user tracked stereoscopic rendering through efficient master/slave synchronization with external multimodal interactions via OSC and VRPN protocols.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="110" name="110"></a>Thursday 1</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Scalable Metadata In- and Output for Multi-platform Data Annotation Applications</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sebastian Pick - Virtual Reality Group, RWTH Aachen University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sascha Gebhardt - Virtual Reality Group, RWTH Aachen University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Bernd Hentschel - Virtual Reality Group, RWTH Aachen University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Torsten W. Kuhlen - Virtual Reality Group, RWTH Aachen University</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author:  Sebastian Pick</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Metadata in- and output are important steps within the data annotation process. However, selecting techniques that effectively facilitate these steps is non-trivial, especially for applications that have to run on multiple virtual reality platforms. Not all techniques are applicable to or available on every system, requiring to adapt workflows on a per-system basis. Here, we describe a metadata handling system based on Android's Intent system that automatically adapts workflows and thereby makes manual adaption needless.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="163" name="163"></a>Thursday 2</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>An Experimental Study on the Virtual Representation of Children</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Ranchida Khantong - University College London         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Xueni Pan - University College London         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Mel Slater - University College London         </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Xueni Pan</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Is it their movements or appearance that helps us to identify a child as a child? We created four video clips with a Virtual Character walking, but with different combinations of either child or adult animation applied on either a child or adult body. An experimental study was conducted with 53 participants who viewed all four videos in random orders. They also reported higher level of empathy, care, and feeling of protection towards the child character as compared to the adult character. Moreover, compared to appearance, animation seems to be playing a bigger role in invoking participants’ emotional responses.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="131" name="131"></a>Thursday 3</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Human-Avatar Interaction and Recognition Memory according to Interaction Types and Methods</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Mingyu Kim - Hanyang University       </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Woncheol Jang - Hanyang University       </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Kwanguk (Kenny) Kim - Hanyang University       </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Mingyu Kim</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: For the several decades, researchers studied human-avatar interactions using a virtual reality (VR). However, speculation on the interaction between a human’s recognition memory and interaction types/methods has not enough considered yet. In the current study, we designed a VR interaction paradigm with two different types of interaction including initiating and responding, and we also included two interaction methods including head-gazing and hand-pointing. The result indicated that there are significant differences in the recognition memory between the initiating and responding interactions. These results suggest that the human-avatar interaction may have similar patterns with the human-human interaction in the recognition memory.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="134" name="134"></a>Thursday 4</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Dynamic Hierarchical Virtual Button-based Hand Interaction for Wearable AR</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Hyejin Kim - Korea Institute of Science and Technology        </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Elisabeth Adelia Widjojo - Korea Institute of Science and Technology        </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jae-In Hwang - Korea Institute of Science and Technology        </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Hyejin Kim</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: This paper presents a novel bare-hand interaction method for wearable AR (augmented reality). The suggested method is using hierarchical virtual buttons which are placed on the image target. Therefore, we can provide precise hand interaction on the image target surface (while using wearable AR). The method operates on a wearable AR system and uses an image target tracker to make occlusion-based interaction button. We introduce the hierarchical virtual button method which is adequate for more precise and faster interaction with augmented objects.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="203" name="203"></a>Thursday 5</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>3D Position Measurement of Planar Photo Detector Using Gradient Patterns</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tatsuya Kodera, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Maki Sugimoto, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Ross Smith, The University of South Australia</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Bruce Thomas, The University of South Australia</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Tatsuya Kodera</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We propose a three dimensional position measurement method employing planar photo detectors to calibrate a Spatial Augmented Reality system of unknown geometry. In Spatial Augmented Reality, projectors overlay images onto an object in the physical environment. For this purpose, the alignment of the images and physical objects is required. Traditional camera based 3D position tracking systems, such as multi-camera motion capture systems, detect the positions of optical markers in two-dimensional image plane of each camera device, so those systems require multiple camera devices at known locations to obtain 3D position of the markers. We introduce a detection method of 3D position of a planar photo detector by projecting gradient patterns. The main contribution of our method is to realize an alignment of the projected images with the physical objects and measuring the geometry of the objects simultaneously for Spatial Augmented Reality applications.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="138" name="138"></a>Thursday 6</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>What Can We Feel on the Back of the Tablet? -- A Thin Mechanism to Display Two Dimensional Motion on the Back and Its Characteristics –</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Itsuo Kumazawa - Imaging Science and Engineering Laboratory, Tokyo Institute of Technology    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Minori Takao - Imaging Science and Engineering Laboratory, Tokyo Institute of Technology    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yusuke Sasaki - Imaging Science and Engineering Laboratory, Tokyo Institute of Technology    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Shunsuke Ono - Imaging Science and Engineering Laboratory, Tokyo Institute of Technology    </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Itsuo Kumazawa</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: The front surface of the tablet computer is dominated by a touch screen and used mostly to display visual information. Under the design, use of the rear surface of the tablet for tactile display is promising as the fingers holding the tablet constantly touch it and feel the feedback steadily. In this paper, a slim design of tactile feedback mechanism that can be easily installed on the back of existing tablets is given and its mechanical performance regarding electricity consumption, latency and force is evaluated. Human capability in perceiving the tactile information on the display is also evaluated.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="156" name="156"></a>Thursday 7</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Cooperation in Virtual Environments with Individual Views</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Vincent Küszter - Technische Universität Chemnitz           </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Guido Brunnett - Technische Universität Chemnitz           </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Daniel Pietschmann - Technische Universität Chemnitz           </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Vincent Küszter</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: When users are interacting collaboratively in a virtual environment it cannot be guaranteed that every user has the same input device or that they have access to the same information. Our research aims at understanding the effects of such asymmetries on the user embodiment in collaborative virtual environments (CVEs). In order to do this, we have developed a prototyping platform for cooperative interaction between two users. To change the information a user has, we are incorporating "special views" for each person. Also, an easily expandable array of input devices is supported.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="133" name="133"></a>Thursday 8</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Interaction with Virtual Agents – Comparison of the participants’ experience between an IVR and a semi-IVR system</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Marios Kyriakou - Department of Computer Science, University of Cyprus             </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Xueni Pan - Institute of Cognitive Neuroscience, University College London             </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yiorgos Chrysanthou - Department of Computer Science, University of Cyprus             </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Marios Kyriakou</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Is our behavior and experience the same in both IVR and in semi-IVR systems when we navigate through a virtual environment populated with virtual agents? Using experiments we show that is more important for the semi-IVR systems to facilitate collision avoidance between the users and the virtual agents accompanied with basic interaction between them. This can increase the sense of presence and make the virtual agents and the environment appear more realistic and lifelike.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="129" name="129"></a>Thursday 9</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Multiple Devices as Windows for Virtual Environment</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jooyoung Lee - Konkuk University         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Hasup Lee - Konkuk University         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">BoYu Gao - Konkuk University         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">HyungSeok Kim - Konkuk University         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jee-In Kim - Konkuk University         </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Jooyoung Lee</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We introduce a method for using multiple devices as windows for interacting with 3-D virtual environment. Provided with life size virtual environment, each device shows a scene of 3-D virtual space on its position and direction. By adopting mobile device to our system, users not only see outer space of stationary screen by relocating their mobile device, but also have personalized view in working space. To acquiring knowledge of device’s pose and orientation, we adopt vision-based approaches. For the last, we introduce an implementation of a system for managing multiple device and letting them have synchronized performance.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="161" name="161"></a>Thursday 10</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Synthesis of Omnidirectional Movie using a Set of Key Frame Panoramic Images</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Roberto Lopez-Gulliver - Ritsumeikan University               </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Takahiro Hatamoto - Ritsumeikan University               </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Kohei Matsumura - Ritsumeikan University               </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Haruo Noma - Ritsumeikan University               </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Roberto Lopez-Gulliver</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We aim to provide an interactive and entertaining environment for users to better enjoy and stay motivated during indoor training. We are now developing a virtual treadmill-based training system allowing users to experience walking or running around various real scenes. A set of key frame 360-degree panoramic images on a grid are used to synthesize, in real-time, an omnidirectional movie for any possible path the user takes. The playback smoothness of the synthesized movie depends on the separation (grid pitch) between key frames. Preliminary experimental results help us determine the largest playback pitch without compromising playback smoothness.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="102" name="102"></a>Thursday 11</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>The Influence of Virtual Reality and Mixed Reality Environments com-bined with two different Navigation Methods on Presence</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Mario Lorenz, Institute for Machine Tools and Production Processes, Technische UniversitÃ¤t Chemnitz</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Marc Busch, Austrian Institute of Technology GmbH</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Loukas Rentzos, Laboratory for Manufacturing Systems and Automation, University of Patras</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Manfred Tscheligi, Austrian Institute of Technology GmbH</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Philipp Klimant, Institute for Machine Tools and Production Processes, Technische UniversitÃ¤t Chemnitz</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Peter Fröhlich, Austrian Institute of Technology GmbH</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Mario Lorenz</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: For various VR/MR/AR applications, such as virtual usability studies, it is very important that the participants have the feeling that they are really in the environment. This feeling of "being" in a mediated environment is described as presence. Two important factors that influence presence are the level of immersion and the navigation method. We developed two navigation methods to simulate natural walking using a Wii Balance Board and a Kinect Sensor. In this preliminary study we examined the effects of these navigation methods and the level of immersion on the participants' perceived presence in a 2x2 factorial between-subjects study with 32 participants in two different VEs (Powerwall and Mixed-Reality-See-Through-Glasses). The results indicate that reported presence is higher for the Kinect navigation and Powerwall for some facets of presence.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="124" name="124"></a>Thursday 12</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Avatar Embodiment Realism and Virtual Fitness Training</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jean-Luc Lugrin - Universität Würzburg, Würzburg, GERMANY    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Maximilian Landeck - Universität Würzburg, Würzburg, GERMANY    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Marc Erich Latoschik - Universität Würzburg, Würzburg, GERMANY    </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Marc Erich Latoschik</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: In this paper we present a preliminary study of the impact of avatar realism on illusion of virtual body ownership (IVBO), when using a full body virtual mirror for fitness training. We evaluated three main types of user representation: realistic and non-realistic avatars as well as no avatar at all. Our results revealed that same-gender realistic human avatar elicited a slightly higher level of illusion and performance. However qualitative analysis of open questions revealed that the feeling of power was higher with non-realistic strong-looking avatars.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="294" name="294"></a>Thursday 13</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Avatar Anthropomorphism and Illusion of Body Ownership in VR</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jean-Luc Lugrin, University of Wuerzburg</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Johanna Latt, University of Wuerzburg</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Marc Erich Latoschik, University of Wuerzburg</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Marc Erich Latoschik</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: In this paper we present a novel experiment to explore the impact of avatar realism on the illusion of virtual body ownership (IVBO) in immersive virtual environments, with full-body avatar embodiment and freedom of movement. We evaluated four distinct avatars (a humanoid robot, a block-man, and both male and female human adult) presenting an increasing level of anthropomorphism in their detailed compositions (specific body's part's shape, scale, dimension, surface topology, texture and colour). Our results revealed that each avatar elicited a relatively high level of illusion. However both machine-like and cartoon-like avatars elicited an equivalent IVBO, slightly superior to the human-ones. A realistic human appearance is therefore not a critical top-down factor of IVBO, and could lead to an Uncanny Valley effect.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="125" name="125"></a>Thursday 14</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Influence of Avatar Realism on Stressful Situation in VR</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jean-Luc Lugrin - Universität Würzburg, Würzburg, GERMANY    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Maximilian Wiedemann - Universität Würzburg, Würzburg, GERMANY    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Daniel Bieberstein - Universität Würzburg, Würzburg, GERMANY    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Marc Erich Latoschik - Universität Würzburg, Würzburg, GERMANY</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">               </span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Marc Erich Latoschik</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: In this paper we present a study of the impact of avatar realism on user experience and performance in stressful immersive virtual environments. We evaluated a stressful and a stress-free environment with partial avatar embodiment under low (iconic) or high (photo- realistic) visual fidelity conditions. An experiment with forty participants did not reveal any significant differences between both graphical versions. This first result represents an interesting find- ing since non realistic avatar and environment representations are faster and more economical to produce while requiring less computational resources.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="119" name="119"></a>Thursday 15</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Extending Touch-less Interaction on Vision Based Wearable Device</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Zhihan Lv - Chinese Academy of Science    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Liangbing Feng - Chinese Academy of Science    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Shengzhong Feng - Chinese Academy of Science    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Haibo Li - Royal Institute of Technology (KTH)      </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Zhihan Lv</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: A touch-less interaction technology on vision based wearable device is designed and evaluated. Users interact with the application with dynamic hands/feet gestures in front of the camera, which triggers the interaction event to interact with the virtual object in the scene. Several proof-of-concept prototypes with eleven dynamic gestures are developed based on the touch-less interaction. At last, a comparing user study evaluation is proposed to demonstrate the usability of the touch-less approach, as well as the impact on user’s emotion, running on a wearable framework or Google Glass.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="303" name="303"></a>Thursday 16</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Blind in a Virtual World: Using Sensory Substitution for Generically Increasing the Accesibilty of Graphical Virtual Environments</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Shachar Maidenbaum, HUJI</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sami Abboud, HUJI</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Galit Buchs, HUJI</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Amir Amedi, HUJI</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Shachar Maidenbaum</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Graphical virtual environments are currently far from accessible to the blind as most of their content is visual. While several previous environment-specific tools have indeed increased accessibility to specific environments they do not offer a generic solution. This is especially unfortunate as such environments hold great potential for the blind, e.g., for safe orientation and learning. Visual-to-audio Sensory Substitution Devices (SSDs) can potentially increase their accessibility in such a generic fashion by sonifying the on-screen content regardless of the specific environment. Using SSDs also taps into the skills gained from using these same SSDs for completely different tasks, including in the real world. However, whether congenitally blind users will be able to use this information to perceive and interact successfully virtually is currently unclear. We tested this using the EyeMusic SSD, which conveys shape and color information, to perform virtual tasks otherwise not possible without vision. We show that these tasks can be accomplished by the congenitally blind.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="230" name="230"></a>Thursday 17</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>I Built It! - Exploring the effects of Customizable Virtual Humans on Adolescents with ASD</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Chao Mei, University of Texas at San Antonio</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Lee Mason, University of Texas at San Antonio</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">John Quarles, University of Texas at San Antonio</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Chao Mei</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Virtual Reality (VR) training games have many potential benefits for autism spectrum disorder (ASD) therapy, such as increasing motivation and improving the abilities of performing daily living activities. Persons with ASD often have deficits in hand-eye coordination, which makes many activities of daily living difficult. A VR game that trains hand-eye coordination could help users with ASD improve their quality of life. Moreover, incorporating users' interests into the game could be a good way to build a motivating game for users with ASD. We propose a Customizable Virtual Human (CVH) which enables users with ASD to easily customize a virtual human and then interact with the CVH in a 3D task. Specifically, we investigated the effects of CVHs with a VR hand-eye coordination training game - Imagination Soccer - and conducted a user study on adolescents with high functioning ASD. We compared the differences of participants' 3D interaction performances, game performances and user experiences (i.e. presence, involvement, and flow) under CVH and Non-customizable Virtual Human (NCVH) conditions. The results indicate that CVHs could effectively improve performance in 3D interaction tasks (i.e., blocking a soccer ball) for users with ASD, motivate them to play the game more, and offer a better user experience.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="304" name="304"></a>Thursday 18</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>The Effects of Olfaction on Training Transfer for an Assembly Task</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Alec Moore, University of Texas at Dallas</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Nicolas Herrera, University of Texas at Dallas</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tyler Hurst, University of Texas at Dallas</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Ryan McMahan, University of Texas at Dallas</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sandra Poeschl, TU Ilmenau</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Ryan P. McMahan</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Context-dependent memory studies have indicated that olfaction, the sense of smell, has a special odor memory that can significantly improve recall in some cases. Virtual reality (VR), which has been investigated as a training tool, could feasibly benefit from odor memory by incorporating olfactory stimuli. There have been a few studies on this concept for semantic learning, but not for procedural training. To address this gap in knowledge, we investigated the effects of olfaction on the transfer of knowledge from training to next-day execution for building a complex LEGO jet-plane model. Our results indicate that the pleasantness of an odor significantly affects training transfer more than whether the encoding and recall contexts match.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="104" name="104"></a>Thursday 19</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>MRI Overlay System Using Optical See-Through for Marking Assistance</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jun Morita - Graduate School of Science and Technology, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sho Shimamura - Graduate School of Science and Technology, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Motoko Kanegae - Graduate School of Science and Technology, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yuji Uema - Graduate School of Media Design, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Maiko Takahashi - Department of Surgery, School of Medicine, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Masahiko Inami - Graduate School of Media Design, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tetsu Hayashida - Department of Surgery, School of Medicine, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Maki Sugimoto - Graduate School of Science and Technology, Keio University</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author:  Jun Morita</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: In this paper we propose an augmented reality system that superimposes MRI onto the patient model. We use a half-silvered mirror and a handheld device to superimpose the MRI onto the patient model. By tracking the coordinates of the patient model and the handheld device using optical markers, we are able to transform the images to the correlated position. Voxel data of the MRI are made so that the user is able to view the MRI from many different angles.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="135" name="135"></a>Thursday 20</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Continuous Automatic Calibration for Optical See-Through Displays</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Kenneth Moser - Mississippi State University       </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yuta Itoh - Technical University Munich     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">J. Edward Swan II - Mississippi State University       </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Kenneth Moser</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: The current advent of consumer level optical see-through (OST) head-mounted displays (HMD's) has greatly broadened the accessibility of Augmented Reality (AR) to not only researchers but also the general public as well. This increased user base heightens the need for robust automatic calibration mechanisms suited for non-technical users. We are developing a fully automated calibration system for two stereo OST HMD's based on the recently introduced interaction free display calibration (INDICA) method. Our current efforts are also focused on the development of an evaluation process to assess the performance of the system during use by non-expert subjects.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="190" name="190"></a>Thursday 21</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Comparing the Performance of Natural, Semi-Natural, and Non-Natural Locomotion Techniques in Virtual Reality</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Mahdi Nabiyouni, Center for Human-Computer Interaction and Department of Computer Science, Virginia Tech</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Ayshwarya Saktheeswaran, Center for Human-Computer Interaction and Department of Computer Science, Virginia Tech</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Doug Bowman, Center for Human-Computer Interaction and Department of Computer Science, Virginia Tech</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Ambika Karanth, Center for Human-Computer Interaction and Department of Computer Science, Virginia Tech</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: One of the goals of much virtual reality (VR) research is to increase realism. In particular, many techniques for locomotion in VR attempt to approximate real-world walking. However, it is not yet fully understood how the design of more realistic locomotion techniques affects user task performance. We performed an experiment to compare a semi-natural locomotion technique (based on the Virtusphere device) with a traditional, non-natural technique (based on a game controller) and a fully natural technique (real walking). We found that the Virtusphere technique was significantly slower and less accurate than both of the other techniques. Based on this result and others in the literature, we speculate that locomotion techniques with moderate interaction fidelity will often have performance inferior to both high-fidelity techniques and well-designed low-fidelity techniques. We argue that our experimental results are an effect of interaction fidelity, and perform an analysis of the fidelity of the three locomotion techniques to support this argument.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="169" name="169"></a>Thursday 22</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Implementation of on-site virtual time machine for mobile devices</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Junichi Nakano - The University of Tokyo              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Takuji Narumi - The University of Tokyo              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tomohiro Tanikawa - The University of Tokyo              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Michitaka His - The University of Tokyo              </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Junichi Nakano</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We developed a system for mobile devices designed to provide a virtual experience of past scenery depicted in old photographs by superimposing them on landscapes in video see-through frames. A user is asked to capture a photograph of a landscape and enter correspondence points between the new and old photos. The old photograph is deformed by projective transformation based on correspondence points. We then superimpose of the old photograph onto video see-through frames of current landscape. To achieve real-time and robust superimposition on mobile devices, both motion-sensor-based pose information and camera-image-keypoint-tracking-based pose information is used for device's camera pose tracking.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="105" name="105"></a>Thursday 23</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>The Effect of Head Mounted Display Weight and Locomotion Method on the Perceived Naturalness of Virtual Walking Speeds</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Niels Christian Nilsson - Aalborg University Copenhagen</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Stefania Serafin - Aalborg University Copenhagen</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Rolf Nordahl - Aalborg University Copenhagen</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Niels Christian Nilsson</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: This poster details a study investigating the effect of Head Mounted Display (HMD) weight and locomotion method (Walking-In-Place and treadmill walking) on the perceived naturalness of virtual walking speeds. The results revealed significant main effects of movement type, but no significant effects of HMD weight were identified.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="150" name="150"></a>Thursday 24</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Third person's footsteps enhanced walking sensation of seated person</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yujiro Okuya - Tokyo Metropolitan University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yasushi Ikei - Tokyo Metropolitan University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tomohiro Amemiya - NTT       </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Koichi Hirota - The University of Tokyo              </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Yujiro Okuya</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We developed an audio-tactile display system to evoke pseudo-walking sensation to a sitting participant. The vibration was added to the heel and toe to imitate cutaneous sensation of the sole during walking. As the auditory stimulus, the sounds of own and another walker's footsteps were provided to the participants through headphones. Only another walker's sound was moved along several trajectories in a VR space. As the result of an experiment conducted to elucidate a sense of walking, third person's sound enhanced not only walking sensation but also translational sensation of a sitting participant.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="151" name="151"></a>Thursday 25</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Does Vibrotactile Intercommunication Increase Collaboration?</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Victor Adriel Oliveira - UFRGS </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Anderson Maciel - UFRGS </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Wilson Sarmiento - Universidad del Cauca </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Luciana Nedel - UFRGS </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">César Collazos - Universidad del Cauca </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Victor Adriel de J. Oliveira</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Communication is a fundamental process in collaborative work. In natural conditions, communication between team members is multimodal. This allows for redundancy, adaptation to different contexts, and different levels of focus. In collaborative virtual environments, however, hardware limitations and lack of appropriate interaction metaphors reduce the amount of collaboration. In this poster, we propose the design and use of a vibrotactile language to improve user intercommunication in CVE and, consequently, to increase the amount of effective collaboration.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="113" name="113"></a>Thursday 26</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Coupled-Clay: Physical-Virtual 3D Collaborative Interaction Environment</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Kasım Özacar - Research Institute of Electrical Communication               </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Takuma Hagiwara - Research Institute of Electrical Communication               </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jiawei Huang - Research Institute of Electrical Communication               </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Kazuki Takashima - Research Institute of Electrical Communication               </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yoshifumi Kitamura - Research Institute of Electrical Communication               </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Kasım Özacar</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Coupled-clay is a bi-directional 3D collaborative interactive environment that enables 3D modeling work between groups of users at two remote spaces; the Physical Interaction Space and the Virtual Interaction Space. The physical Interaction Space enables a user to directly manipulate a physical object whose shape and position are precisely tracked. The shape is transferred to the virtual interaction space where users observe the virtual shape which corresponds to the physical object, and manipulate its geometrical and graphical attributes using a multi-user stereoscopic tabletop display. The manipulations are reflected to the physical object by a robotic arm and a top-mounted projector.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="204" name="204"></a>Thursday 27</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>GPU-accelerated Attention Map Generation for Dynamic 3D Scenes</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Thies Pfeiffer, CITEC, Faculty of Technology, Bielefeld University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Cem Memili, Faculty of Technology, Bielefeld University</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Thies Pfeiffer</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Measuring visual attention has become an important tool during product development. Attention maps are important qualitative visualizations to communicate results within the team and to stakeholders. We have developed a GPU-accelerated approach which allows for real-time generation of attention maps for 3D models that can, e.g., be used for on-the-fly visualizations of visual attention distributions and for the generation of heat-map textures for offline high-quality renderings. The presented approach is unique in that it works with monocular and binocular data, respects the depth of focus, can handle moving objects and is ready to be used for selective rendering.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="123" name="123"></a>Thursday 28</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>A Procedure for Accurate Calibration of a Tabletop Haploscope AR Environment</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Nate Phillips - Mississippi State University       </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">J. Swan - Mississippi State University       </span></span></p>
<p> </p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Nate Phillips</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: In previous papers, we have reported a novel haploscope-based augmented reality (AR) display system. The haploscope allows us to precisely set various optical display parameters, in order to study the interaction between optical and graphical display properties on such perceptual issues as depth presentation. While using the haploscope, it became clear that we needed to develop novel calibration procedures, both because of the novelty of the haploscope’s optical design, and also because of the required accuracy on the order of 1 mm. This poster proposes novel calibration procedures based on the use of perpendicular laser fans.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="140" name="140"></a>Friday 1</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Using Astigmatism in Wide Angle HMDs to Improve Rendering</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Daniel Pohl, Intel Corporation</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Timo Bolkart, Saarland University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Stefan Nickels, Intel Visual Computing Institute</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Oliver Grau, Intel Corporation</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Daniel Pohl</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Lenses in modern consumer HMDs introduce distortions like astigmatism: only the center area of the displayed content can be perceived sharp while with increasing distance from the center the image gets out of focus. We show with three new approaches that this undesired side effect can be used in a positive way to save calculations in blurry areas. For example, using sampling maps to lower the detail in areas where the image is blurred through astigmatism, increases performance by a factor of 2 to 3. Further, we introduce a new calibration of user-specific viewing parameters that increase the performance by about 20-75%.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="109" name="109"></a>Friday 2</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Shark Punch: A Virtual Reality Game for Aquatic Rehabilitation</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">John Quarles, University of Texas at San Antonio</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: John Quarles</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We present a novel underwater VR game - Shark Punch - in which the user must fend off a virtual Great White shark with real punches in a real underwater environment. This poster presents our underwater VR system and our iterative design process through field tests with a user with disabilities. We conclude with proposed usability,accessibility, and system design guidelines for future underwater VR rehabilitation games.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="157" name="157"></a>Friday 3</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Real-time SLAM for static multi-objects learning and tracking applied to augmented reality applications</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Datta Ramadasan, Institut Pascal</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Marc Chevaldonne, ISIT</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Thierry Chateau, Institut Pascal</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Ramadasan</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: This paper presents a new approach for multi-objects tracking from a video camera moving in an unknown environment. The tracking involves static objects of different known shapes, whose poses and sizes are determined online. For augmented reality applications, objects must be precisely tracked even if they are far from the camera or if they are hidden. Camera poses are computed using simultaneous localization and mapping (SLAM) based on bundle adjustment process to optimize problem parameters. We propose to include in an incremental bundle adjustment the parameters of the observed objects as well as the camera poses and 3D points. We show, through the example of 3D models of basics shapes (planes, parallelepipeds, cylinders and spheres) coarsely initialized online using a manual selection, that the joint optimization of parameters constrains the 3D points to approach the objects, and also constrains the objects to fit the 3D points. Moreover, we developed a generic and optimized library to solve this modified bundle adjustment and demonstrate the high performance of our solution compared to the state of the art alternative. Augmented reality experiments in real-time demonstrate the accuracy and the robustness of our method.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="109" name="109"></a>Friday 4</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Social Presence with Virtual Glass</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Holger Regenbrecht - University of Otago</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Mohammed Alghamdi - University of Otago</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Simon Hoermann - University of Otago</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tobias Langlotz - University of Otago</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Mike Goodwin - University of Otago</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Colin Aldridge - University of Otago</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Holger Regenbrecht</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We introduce the concept of a virtualized version of Google Glass called Virtual Glass. Virtual Glass is integrated into our collaborative virtual environment as a real-world metaphor for a communication device, one particularly suited for instructor-performer systems.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="297" name="297"></a>Friday 5</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Semi-automatic Calibration of a Projector-Camera System Using Arbitrary Objects With Known Geometry</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Christoph Resch, EXTEND3D GmbH</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Peter Keitler, EXTEND3D GmbH</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Christoffer Menk, Volkswagen AG</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Gudrun Klinker, TU München</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Christoph Resch</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We propose a new semi-automatic calibration approach for projector-camera systems that - unlike existing auto-calibration approaches - additionally recovers the necessary global scale by projecting on an arbitrary object of known geometry from one view. Our method therefore combines surface registration with bundle adjustment optimization on points reconstructed from structured light projections. In simulations on virtual data and experiments with real data we demonstrate that our approach estimates the global scale robustly and is furthermore able to improve incorrectly guessed intrinsic and extrinsic calibration parameters.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="175" name="175"></a>Friday 6</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Navigation in REVERIE's Virtual Environments</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Fiona Rivera, Queen Mary University of London</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Fons Kuijk, CWI Amsterdam</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Ebroul Izquierdo, Queen Mary University of London</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Fiona M Rivera</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: This work presents a novel navigation system for social collaborative virtual environments populated with multiple characters. The navigation system ensures collision free movement of avatars and agents. It supports direct user manipulation, automated path planning, positioning to get seated, and follow-me behaviour for groups. In follow-me mode, the socially aware system manages the mise en place of individuals within a group. A use case centred around on an educational virtual trip to the European Parliament created for the REVERIE FP7 project, also serves as an example to bring forward aspects of such navigational requirements.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="141" name="141"></a>Friday 7</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Collaborative Telepresence Workspaces for Space Operation and Science</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">David Roberts - University of Salford     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Arturo Garcia - University of Salford     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Janki Dodiya - Deutsches Zentrum für Luft- und Raumfahrt e.V (DLR)               </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Robin Wolf - Deutsches Zentrum für Luft- und Raumfahrt e.V (DLR)               </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Allen Fairchild - University of Salford     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Terrence Fernando - University of Salford     </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: David Roberts</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We introduce the collaborative telepresence workspaces for SPACE operation and science that are under development in the European research project CROSS DRIVE. The vision is to give space mission controllers and scientists the impression of “beaming” to the surface of Mars, along with simulations of the environment and equipment, to step out together where a robot has or may move. We briefly overview the design and describe the state of the demonstrator. The contribution of the publication is to give an example of how collaborative Virtual Reality research is being taken up in space science.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="130" name="130"></a>Friday 8</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Does Virtual Reality really affect visual perception of egocentric distance?</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Thomas Rousset - Aix Marseille Universite              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Christophe Bourdin - Aix Marseille Universite              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Cedric Goulon - CNRS    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jocelyn Monnoyer - PSA Peugeot Citroen    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jean-Louis Vercher - CNRS    </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Thomas Rousset</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Driving simulators are used to study human behavior in mobility. The aim of this study was to determine the effect of interactive factors (stereoscopy and motion parallax) on distance perception. After a training session, participants were asked to estimate the relative location of a car on the same road. Results suggest that distance perception does not depend on interactive factors. However, the study revealed a large interpersonal variability: two profiles of participants were defined, those who accurately perceived distances and those who underestimated distances as usually reported. This classification was correlated to the level of performance during the training phase.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="288" name="288"></a>Friday 9</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>A GPU-Based Adaptive Algorithm for Non-Rigid Surface Registration</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Antonio Carlos dos Santos Souza, Federal Institute of Bahia</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Márcio Cerqueira de Farias Macedo, Federal University of Bahia</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Antonio Lopes Apolinario Junior, Federal University of Bahia</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Antonio Carlos dos Santos Souza</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Non-rigid surface registration is fundamental when accurate tracking or reconstruction of 3D deformable shapes is desirable. However, the majority of non-rigid registration methods are not as fast as the ones developed in the field of rigid registration. Fast methods for non-rigid surface registration are particularly interesting for markerless augmented reality applications, in which the object being used as marker can support non-rigid user interaction. In this paper, we present an adaptive algorithm for non-rigid surface registration. Taking advantage from this adaptivity and the parallelism of the GPU, we show that the proposed algorithm is capable to achieve near real-time performance with an approach as accurate as the ones proposed in the literature.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="146" name="146"></a>Friday 10</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Characteristics of virtual walking sensation created by a 3-dof motion seat</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Seiya Shimabukuro - TMU     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Shunki Kato - TMU     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yasushi Ikei - TMU     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Koichi Hirota - U-Tokyo             </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tomohiro Amemiya - NTT       </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Michiteru Kitazaki - TUT       </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Shunki Kato</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Rendering characteristics of a virtual walk are presented. A motion seat created a small body motion passively in three dof (lift, roll, and pitch directions) to make the user feel as if the user him-/herself was walking despite actually sitting own body. We consider the actual self-body is a medium to render the virtual body to share experiences of others by using the motion seat. Basic characteristics of perception levels of the virtual walk were measured and compared. The result shows that perception levels of the virtual walk by the motion seat were around those of a real walk.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="214" name="214"></a>Friday 11</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Self-Characterstics and Sound in Immersive Virtual Reality - Estimating Avatar Weight from Footstep Sounds</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Erik Sikström, Aalborg University Copenhagen</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Amalia de Götzen, Aalborg University Copenhagen</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Stefania Serafin, Aalborg University Copenhagen</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Erik Sikström</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: This experiment aimed to investigate whether a user controlling a full body avatar via real time motion tracking in an immersive virtual reality setup, would estimate the weight of the virtual avatar differently if the footstep sounds are manipulated using three different audio filter settings. The visual appearance of the avatar was available in two sizes. The subjects performed six walks with each audio configuration active once over two ground types. After completing each walk, the participants were asked to estimate the weight of the virtual avatar and the suitability of the audio feedback. The results indicate that the filters amplifying the two lower center frequencies altered the subjects' estimates of the weight of the avatar body towards being heavier than when compared with the filter with the higher center frequency. There were no significant differences between the weight estimates of the two groups using the different avatar bodies.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="213" name="213"></a>Friday 12</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Wings and Flying in Immersive VR - Controller Type, Sound Effects and Experienced Ownership and Agency</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"> Erik Sikström, Aalborg University Copenhagen</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Amalia de Götzen, Aalborg University Copenhagen</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Stefania Serafin, Aalborg University Copenhagen</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Erik Sikström</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: An experiment investigated the subjective experiences of ownership and agency of a pair of virtual wings attached to a motion controlled avatar in an immersive virtual reality setup. A between groups comparison of two ways of controlling the movement of the wings and flight ability. One where the subjects achieved the wing motion and flight ability by using a hand-held video game controller and the other by moving the shoulder. Through four repetitions of a flight task with varying amounts of self-produced audio feedback (from the movement of the virtual limbs), the subjects evaluated their experienced embodiment of the wings on a body ownership and agency questionnaire. The results shows significant differences between the controllers in some of the questionnaire items and that adding self-produced sounds to the avatar, slightly changed the subject's evaluations.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="158" name="158"></a>Friday 13</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Optical See-through HUDs Effect on Depth Judgments of Real World Objects</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Missie Smith - Virginia Tech     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Nadejda Doutcheva - Virginia Tech     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Joseph Gabbard - Virginia Tech     </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Gary Burnett - University of Nottingham          </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Missie Smith</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: While AR HUD graphics offer opportunities for improved performance and safety, there exists a need to determine the effects of such graphics on human perception and workload. We are especially interested in examining this problem within the domain of surface transportation (e.g., driving). This work represents an initial step in understanding how AR graphics intended to visually cue driving hazards (e.g., pedestrian) may affect drivers’ depth judgments to real world driving hazards. This study explores whether Augmented Reality (AR) graphics have directional effects on users’ depth perception of real-world objects.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="117" name="117"></a>Friday 14</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>EVE: Exercise in Virtual Environments</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Amaury SOLIGNAC - I.C.E.B.E.R.G.    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sebastien KUNTZ - MiddleVR          </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Amaury SOLIGNAC</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: EVE (Exercise in Virtual Environments) is an operational VR system designed for space, polar and submarine crews. This system allows crewmembers -living and working in artificial habitats- to explore immersive natural landscapes during their daily physical exercise, and experience presence in a variety of alternate environments. Using recent hardware and software, this innovative psychological counter-measure aims at reducing the adverse effects of confinement and monotony in long duration missions, while maintaining motivation for physical exercise. Initial testing with a proof-of-concept prototype was conducted near the South magnetic pole, as well as in transient microgravity.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="139" name="139"></a>Friday 15</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Subjective Evaluation of Peripheral Viewing during Exposure to a 2D/3D Video Clip</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Masumi Takada - Aichi Medical University             </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Masaru Miyao - Nagoya University         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Hiroki Takada - University of Fukui        </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Masumi Takada</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: The present study examines the effects of peripheral vision on reported motion sickness during exposure to 2D/3D video clips for 1 minute and for 1 minute afterwards in human subjects. The Simulator Sickness Questionnaire was administered after exposure to the video clips with or without visual pursuit of a 3D object and compared. The questionnaire findings significantly changed after the subjects viewed the video clips peripherally. This influence may result when subjects view a poorly depicted background element peripherally, which generates depth perception that contradicts daily experience.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="161" name="161"></a>Friday 16</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Zoom Factor Compensation for Monocular SLAM</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Takafumi Taketomi, Nara Institute of Science and Technology</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Janne Heikkilä, University of Oulu</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Takafumi Taketomi</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: SLAM algorithms are widely used in augmented reality applications for registering virtual objects. Most SLAM algorithms estimate camera poses and 3D positions of feature points using known intrinsic camera parameters that are calibrated and fixed in advance. This assumption means that the algorithm does not allow changing the intrinsic camera parameters during runtime. We propose a method for handling focal length changes in the SLAM algorithm. Our method is designed as a pre-processing step for the SLAM algorithm input. In our method, the change of the focal length is estimated before the tracking process of the SLAM algorithm. Camera zooming effects in the input camera images are compensated for by using the estimated focal length change. By using our method, camera zooming can be used in the existing SLAM algorithms such as PTAM with minor modifications. In the experiment, the effectiveness of the proposed method was quantitatively evaluated. The results indicate that the method can successfully deal with abrupt changes of the camera focal length.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="308" name="308"></a>Friday 17</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>A Modified Tactile Brush Algorithm for Complex Touch Gestures</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Fei Tang, University of Texas at Dallas</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Ryan McMahan, University of Texas at Dallas</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Eric Ragan, Oak Ridge National Laboratory</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tandra Allen, University of Texas at Dallas</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Ryan P. McMahan</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Several researchers have investigated phantom tactile sensation (i.e., the perception of a nonexistent actuator between two real actuators) and apparent tactile motion (i.e., the perception of a moving actuator due to time delays between onsets of multiple actuations). Prior work has focused primarily on determining appropriate Durations of Stimulation (DOS) and Stimulus Onset Asynchronies (SOA) for simple touch gestures, such as a single finger stroke. To expand upon this knowledge, we investigated complex touch gestures involving multiple, simultaneous points of contact, such as a whole hand touching the arm. To implement complex touch gestures, we modified the Tactile Brush algorithm to support rectangular areas of tactile stimulation.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="265" name="265"></a>Friday 18</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Experiencing Interior Environments: New Approaches for the Immersive Display of Large-Scale Pointcloud Data</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Ross Tredinnick, University of Wisconsin - Madison</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Markus Broecker, University of Wisconsin - Madison</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Kevin Ponto, University of Wisconsin - Madison</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Kevin Ponto</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: This document introduces a new application for rendering massive LiDAR point cloud data sets of interior environments within high-resolution immersive VR display systems. Overall contributions are: to create an application which is able to visualize large-scale point clouds at interactive rates in immersive display environments, to develop a flexible pipeline for processing LiDAR data sets that allows display of both minimally processed and more rigorously processed point clouds, and to provide visualization mechanisms that produce accurate rendering of interior environments to better understand physical aspects of interior spaces. The work introduces three problems with producing accurate immersive rendering of LiDAR point cloud data sets of interiors and presents solutions to these problems. Rendering performance is compared between the developed application and a previous immersive LiDAR viewer.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="164" name="164"></a>Friday 19</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Landscape Change From Daytime To Nighttime Under Augmented Reality Environment</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Noriyuki Uda - Nagoya Sangyo University         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yoshitaka Kamiya - Nagoya Sangyo University         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Mamoru Endo - Nagoya University         </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Takami Yasuda - Nagoya University         </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Noriyuki Uda</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: AR (Augmented Reality) technology, which uses actual landscape photography captured with a camera on-site as a base and then overlays virtual objects on that landscape image, is valid for landscape simulation. In this study, we generate nightscape images by superimposing high-luminance segments (virtual objects) over darkening landscape images. They are adjusted due to dark adaption modification. We found that the nightscape was evaluated more highly than the daytime landscape, and we were able to quantitatively confirm that evaluations were high for landscape in twilight after sundown.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="152" name="152"></a>Friday 20</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Impact of Illusory Resistance on Finger Walking Behavior</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yusuke Ujitoko - The University of Tokyo              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Koichi Hirota - The University of Tokyo              </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Yusuke Ujitoko</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We aim to enable additional sensation when using an anthropomorphic finger motion interface. To do so, we applied a conventional method to generate pseduo-haptics. To control the amount of scroll resulting from finger displacement on a display, illusory resistance or pseudo friction was confirmed by subjective evaluation. We first clarified that this illusory resistance influences finger walking behavior such as stride or speed. An additional experiment conducted in public space verified this influence.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="154" name="154"></a>Friday 21</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Development of a Wearable Haptic Device with</strong>　<strong>Pneumatic Artificial Muscles and MR brake</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Makasazu Egawa - chuo-university              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Takumi Watanabe - chuo-university              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Taro Nakamura - chuo-university              </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Masakazu Egawa</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Desktop haptic device has been developed in the field of rehabilitation and entertainment. However, the desktop type restrains human’s movement. Therefore, it is difficult to receive force sense information, moving to wide range position and posture. In this study, we developed a 1-DOF wearable haptic device with pneumatic artificial muscles and a MR brake. These smart actuators have high power density and change its output force structurally. Therefore, this haptic device can render various force sense such as elasticity, friction and viscosity. We describe two experiments rendering elasticity and friction to evaluate the performance of the device.</span></span></p>
<p> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="272" name="272"></a>Friday 22</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Preliminary Evaluation of a Virtual Needle Insertion Training System</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Duc Van NGUYEN, University of Evry</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Safa Ben Lakhal, University of Evry</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Amine Chellali, University of Evry</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Amine Chellali</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Inserting a needle to perform a biopsy requires a high haptic sensitivity. The traditional learning methods based on observation and training on real patients are questionable. In this paper, we present a preliminary evaluation of a VR trainer for needle insertion tasks. The system aims to replicate an existing physical setup while overcoming some of its limitations. Results permit to validate some design choices and suggest some UI improvements.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="114" name="114"></a>Friday 23</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>From visual cues to climate perception in virtual urban environments</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Toinon Vigier - CERMA UMR CNRS 1563             </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Guillaume Moreau - CERMA UMR CNRS 1563             </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Daniel Siret - CERMA UMR CNRS 1563             </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Toinon Vigier</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Virtual reality is a good tool to design and assess urban projects and to study perception in cities. Climate perception significantly influences the perception and use of urban spaces; however, virtual urban environments are scarcely represented with different climatic aspects. In this paper, we study the role that visual cues (sky aspect, shadows, sun location, and light effects) specifically play in climate perception (season, daytime and temperature) in virtual urban environments. We present and discuss the data we collected from a recent virtual reality experiment in which ten variations of the climatic context in the same urban space were assessed.</span></span></p>
<p> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="218" name="218"></a>Friday 24</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>HorizontalDragger: a Freehand Remote Selector for Object Acquisition</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Siju Wu, IBISC, Evry University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Samir Otmane, IBISC, Evry University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Amine Chellali, IBISC, Evry University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Guillaume Moreau, CERMA, Ecole Centrale de Nantes</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Siju Wu</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Interaction with computers using freehand gestures becomes more and more popular. However, it is hard to make precise inputs by making gestures in air without a physical support. In this paper, we present HorizontalDragger, a new selection technique aimed to improve the selection precision by converting the 2D selection problem into a 1D selection problem. After setting a region of interest in the display, all the objects inside the region are considered as potential targets. The user can drag the index finger horizontally to choose the desired target.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="136" name="136"></a>Friday 25</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>A Real-Time Welding Training System Based on Virtual Reality</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Benkai Xie, Wuhan University of Technology</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Qiang Zhou, Wuhan University of Technology</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Liang Yu, Wuhan Onew Technology Co.,Lid</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Benkai Xie</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Onew360 is a training simulator for simulating gas metal arc welding (GMAW). This system is comprised of standard welding hardware components (helmet, gun, work-piece), a PC, a head-mounted display, a tracking system for both the torch and the user's head, and external audio speakers. The track model of welding simulator using single-camera vision measurement technology to calculate the position of the welding gun and helmet, and the simulation model using simple model method to simulate the weld geometry based on the orientation and speed of the welding torch. So that the system produce a realistic, interactive, and immersive welding experience.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="234" name="234"></a>Friday 26</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Transparent Cockpit Using Telexistence</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Takura Yanagi, Nissan Motor Co</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Charith Lasantha Fernando, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">MHD Yamen Saraiji, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Kouta Minamizawa, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Susumu Tachi, Keio University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Norimasa Kishi, Nissan Motor Co</span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Takura Yanagi</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: We propose an indirect-vision, video-see-through augmented reality (AR) cockpit that uses telexistence technology to provide an AR enriched, virtually transparent view of the surroundings through monitors instead of windows. Such a virtual view has the potential to enhance driving performance and experience above conventional glass as well as head-up display equipped cockpits by combining AR overlays with images obtained by future image sensors that are superior to human eyes. As a proof of concept, we replaced the front windshield of an experimental car by a large stereoscopic monitor. A robotic stereo camera pair that mimics the driver's head motions provides stereoscopic images with seamless motion parallax to the monitor. Initial driving tests at moderate speeds on roads within our research facility confirmed the illusion of transparency. We will conduct human factors evaluations after implementing AR functions in order to show whether it is possible to achieve an overall benefit over conventional cockpits in spite of possible conceptual issues like latency, shift of viewpoint and short distance between driver and display.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="171" name="171"></a>Friday 27</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Flying Robot Manipulation System Using a Virtual Plane</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Kazuya Yonezawa - The University of Tokyo              </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Takefumi Ogawa - The University of Tokyo              </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Kazuya Yonezawa</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: The flexible movements of flying robots make it difficult for novices to manipulate them precisely with controllers such as a joystick. Moreover, the mapping of instructions between a robot and its reactions is not necessarily intuitive for users. We propose manipulation methods for flying robots using augmented reality technologies. In the proposed system, a virtual plane is superimposed on a flying robot and users control the robot by manipulating the virtual plane and drawing a moving path on it. We present the design and implementation of our system and describe experiments conducted to evaluate our methods.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="182" name="182"></a>Friday 28</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Binocular Interface: Interaction Techniques Considering Binocular Parallax for a Large Display</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Keigo Yoshimura - Graduate school of Interdisciplinary Information Studies, The University of Tokyo        </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Takefumi Ogawa - Information Technology Center, The University of Tokyo          </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Keigo Yoshimura</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: There have been many studies on intuitive user interfaces for large displays by using pointing movements. However, if a user cannot reach a display, object manipulations on the display are difficult because the user will see duplicate fingers due to binocular parallax. We propose Binocular Interface, which enables interactions with an object by using two pseudo fingers. In a prototype, pointing positions on the display are estimated on the basis of the positions of eyes and a finger detected by an RGB-D camera. We implemented three basic operations (select, move, and resize) using duplicate fingers and evaluated each operation. </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">________________________________________</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><a id="136" name="136"></a>Friday 29</span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Tracking Human Locomotion by Relative Positional Feet Tracking</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Markus Zank - Innovation Center Virtual Reality, ETH Zurich    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Thomas Nescher - Innovation Center Virtual Reality, ETH Zurich    </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Andreas Kunz - Innovation Center Virtual Reality, ETH Zurich    </span></span></p>
<p> </p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Presenting Author: Markus Zank</span></span></p>
<p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: Tracking human movements and locomotion accurately in real time requires expensive tracking systems which need a lot of time to install and their cost typically increases with the size of the tracked space. This poster presents an approach to significantly reduce costs for tracking human locomotion in large tracking spaces. The proposed approach employs a low-cost user-worn tracking system to track limbs including the user’s feet in a user-centric coordinate-system. This relative limb position is associated with the absolute position in a given environment by locking a foot's global position while it is in the stance-phase of the gait cycle.</span></span></p>
</div></div></div>  </div>

      <footer>
                </footer>
  
    </div>
  
</div> <!-- /.block -->
</div>
  </section> <!-- /#main -->
  </div>

      <aside id="sidebar-second" role="complementary" class="sidebar clearfix">
     <div class="region region-sidebar-second">
  <div id="block-block-6" class="block block-block">

      
  <div class="content">
    <p class="rtecenter"><span style="font-family:lucida sans unicode,lucida grande,sans-serif"><span style="font-size:18px"><strong><a href="indexe737.html?q=node/51#overlay-context=node/51%3Fq%3Dnode/51" target="_blank">Exhibitors and Supporters</a></strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><strong><span style="font-size:16px"><span style="font-family:arial,helvetica,sans-serif">Platinum Level</span></span></strong></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/INRIA_SCIENTIFIQUE_UK_CMJN-180x65.png" style="height:65px; line-height:20.7999992370605px; text-align:center; width:180px" width="180" height="65" /></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/nsf-180x180.jpg" style="height:180px; width:180px" width="180" height="180" /></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:16px"><span style="font-family:arial,helvetica,sans-serif"><strong>Silver Level</strong></span></span></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/AFRV_small-180x63.png" style="height:63px; width:180px" width="180" height="63" /></p>
<p class="rtecenter"><span style="font-size:16px"><img alt="" src="sites/default/files/resize/Logo-AGP_carr%c3%a9-180x180.jpg" style="height:180px; width:180px" width="180" height="180" /></span></p>
<p class="rtecenter"><span style="font-size:16px"><span style="font-family:arial,sans-serif"><img alt="" src="sites/default/files/resize/ART-logo%20-%20black-180x68.png" style="height:68px; width:180px" width="180" height="68" /></span></span></p>
<p class="rtecenter"><span style="font-size:16px"><img alt="" src="sites/default/files/resize/Blue%20Vicon%20Logo-180x42.jpg" style="height:42px; width:180px" width="180" height="42" /></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif"><span style="font-size:16px"><strong>Bronze Level</strong></span></span></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/3DRudd_logo_Coul-180x65.png" style="height:65px; width:180px" width="180" height="65" /></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/logo_GENESIS%20Haute%20D%c3%a9finition-180x42.jpg" style="height:42px; line-height:1.6; width:180px" width="180" height="42" /></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/haption_1-180x121.jpg" style="font-size:16px; height:121px; line-height:1.6; width:180px" width="180" height="121" /></p>
<p class="rtecenter"><span style="font-size:16px"><img alt="" src="sites/default/files/resize/MiddleVR-180x127.png" style="height:127px; width:180px" width="180" height="127" /></span></p>
<p class="rtecenter"><span style="font-size:16px"><img alt="" src="sites/default/files/resize/okta-180x74.png" style="height:74px; width:180px" width="180" height="74" /></span></p>
<p class="rtecenter"><span style="font-size:16px"><span style="font-family:calibri,sans-serif"><img alt="" src="sites/default/files/resize/razer-sensics-180x50.png" style="height:50px; width:180px" width="180" height="50" /></span></span></p>
<p class="rtecenter"><span style="font-size:16px"><img alt="" src="sites/default/files/resize/Logo_Technicolor_Q-180x69.png" style="height:69px; width:180px" width="180" height="69" /></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:16px"><span style="font-family:arial,helvetica,sans-serif"><strong>Publishers</strong></span></span></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/PRES_full_logo_gray-180x63.gif" style="height:63px; width:180px" width="180" height="63" /></p>
<p class="rtecenter"><span style="font-size:16px"><strong><img alt="" src="sites/default/files/resize/Springer_cmyk-180x48.png" style="height:48px; width:180px" width="180" height="48" /></strong></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:16px"><span style="font-family:arial,helvetica,sans-serif"><strong>Supporters</strong></span></span></p>
<p class="rtecenter"><span style="font-size:16px"><span style="font-family:arial,helvetica,sans-serif"><strong><img alt="" src="sites/default/files/resize/disney-180x59.png" style="height:59px; width:180px" width="180" height="59" /></strong></span></span></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/egbanner_0-180x31.gif" style="height:31px; width:180px" width="180" height="31" /></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/egfr-logo_0-180x31.jpg" style="height:31px; width:180px" width="180" height="31" /></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/Persyval-lab_0-180x44.jpg" style="height:44px; width:180px" width="180" height="44" /></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:16px"><span style="font-family:arial,helvetica,sans-serif"><strong>Local Supporters</strong></span></span></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/logocg13%20-%20-180x77.jpg" style="height:77px; width:180px" width="180" height="77" /></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/logo_culturespace-180x32.jpg" style="height:32px; width:180px" width="180" height="32" /></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/Ibis%20style-180x181.png" style="height:181px; width:180px" width="180" height="181" /></p>
<p class="rtecenter"> </p>
<p class="rtecenter"> </p>
<p class="rtecenter"> </p>
  </div>
  
</div> <!-- /.block -->
</div>
    </aside>  <!-- /#sidebar-second -->
   </div>
</div> 


   

       <div id="footer">
      <div class="footer_wrapper">
                     <div class="copyright">Copyright IEEE VR 2015 </div>
                </div>
  </div>
     </div>
</div>
  <script type="text/javascript" src="modules/statistics/statistics6d18.js?njikyt"></script>
</body>

<!-- Mirrored from www.ieeevr.org/2015/?q=node/49 by HTTrack Website Copier/3.x [XR&CO'2013], Tue, 24 Mar 2015 14:09:22 GMT -->
</html>
