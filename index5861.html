<!DOCTYPE html>
<html lang="en-US">

<!-- Mirrored from www.ieeevr.org/2015/?q=node/33 by HTTrack Website Copier/3.x [XR&CO'2013], Tue, 24 Mar 2015 14:09:22 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta about="/2015/?q=node/33" property="sioc:num_replies" content="0" datatype="xsd:integer" />
<link rel="shortcut icon" href="sites/default/files/icon.png" type="image/png" />
<meta content="Research Demos" about="/2015/?q=node/33" property="dc:title" />
<link rel="shortlink" href="index5861.html?q=node/33" />
<meta name="Generator" content="Drupal 7 (http://drupal.org)" />
<link rel="canonical" href="index5861.html?q=node/33" />

<title>Research Demos | IEEE VR 2015</title>
<style type="text/css" media="all">@import url("modules/system/system.base6d18.css?njikyt");
@import url("modules/system/system.menus6d18.css?njikyt");
@import url("modules/system/system.messages6d18.css?njikyt");
@import url("modules/system/system.theme6d18.css?njikyt");</style>
<style type="text/css" media="all">@import url("modules/comment/comment6d18.css?njikyt");
@import url("modules/field/theme/field6d18.css?njikyt");
@import url("modules/node/node6d18.css?njikyt");
@import url("modules/search/search6d18.css?njikyt");
@import url("modules/user/user6d18.css?njikyt");
@import url("modules/ckeditor/css/ckeditor6d18.css?njikyt");</style>
<style type="text/css" media="all">@import url("themes/business_enterprises/style6d18.css?njikyt");</style>
<script type="text/javascript" src="modules/jquery_update/replace/jquery/1.7/jquery.minc011.js?v=1.7.1"></script>
<script type="text/javascript" src="misc/jquery.once7839.js?v=1.2"></script>
<script type="text/javascript" src="misc/drupal6d18.js?njikyt"></script>
<script type="text/javascript" src="themes/business_enterprises/js/superfish6d18.html?njikyt"></script>
<script type="text/javascript" src="themes/business_enterprises/js/effects6d18.html?njikyt"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
jQuery.extend(Drupal.settings, {"basePath":"\/2015\/","pathPrefix":"","ajaxPageState":{"theme":"business_enterprises","theme_token":"eTsBltHJb7SMv7fayqhwEvGyeDlBhjnos_AjbbkfZ5c","js":{"modules\/statistics\/statistics.js":1,"modules\/jquery_update\/replace\/jquery\/1.7\/jquery.min.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"themes\/business_enterprises\/js\/superfish.js":1,"themes\/business_enterprises\/js\/effects.js":1},"css":{"modules\/system\/system.base.css":1,"modules\/system\/system.menus.css":1,"modules\/system\/system.messages.css":1,"modules\/system\/system.theme.css":1,"modules\/comment\/comment.css":1,"modules\/field\/theme\/field.css":1,"modules\/node\/node.css":1,"modules\/search\/search.css":1,"modules\/user\/user.css":1,"modules\/ckeditor\/css\/ckeditor.css":1,"themes\/business_enterprises\/style.css":1}},"statistics":{"data":{"nid":"33"},"url":"\/2015\/?q=modules\/statistics\/statistics.php"}});
//--><!]]>
</script>
<!--[if lt IE 9]><script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
</head>
<body class="html not-front not-logged-in one-sidebar sidebar-second page-node page-node- page-node-33 node-type-page">
    <div id="wrapper">
  <div class="inwrap">
    <div id="header-top">
        <div class="logo">
                     <div id="logoimg">
            <a href="index.html" title="Home"><img src="sites/default/files/vr15.png" alt="Home" /></a>
            </div>
                   <div class="sitename">
            <h1><a href="index.html" title="Home"></a></h1>
            <h2></h2>
         </div>
      </div>
	
    <div id="menu-container">
    <div id="main-menu" class="menu-menu-container">
        <ul class="menu"><li class="first leaf"><a href="index.html">Home</a></li>
<li class="expanded active-trail"><a href="index5ffe.html?q=node/6" class="active-trail">Program</a><ul class="menu"><li class="first leaf"><a href="indexf6ef.html?q=node/48">Program</a></li>
<li class="leaf"><a href="index01ce.html?q=node/25">VR Keynote Speaker</a></li>
<li class="leaf"><a href="indexec53.html?q=node/47">Papers</a></li>
<li class="leaf"><a href="index7c87.html?q=node/31">Panels</a></li>
<li class="leaf"><a href="indexb963.html?q=node/39">Workshop Papers</a></li>
<li class="leaf"><a href="indexa7ca.html?q=node/46">Industrial Presentations</a></li>
<li class="leaf"><a href="index4920.html?q=node/45">Lab/Project Presentations</a></li>
<li class="leaf"><a href="index6f9b.html?q=node/49" title="Attempt to re-order the posters, and regroup them into three sets (Wednesday, Thursday, Friday)">Posters</a></li>
<li class="leaf active-trail"><a href="index5861.html?q=node/33" class="active-trail active">Research Demos</a></li>
<li class="leaf"><a href="index6da2.html?q=node/30">Tutorials</a></li>
<li class="last leaf"><a href="indexe737.html?q=node/51">Exhibitors</a></li>
</ul></li>
<li class="expanded"><a href="index763b.html?q=node/5">Call for Participation</a><ul class="menu"><li class="first leaf"><a href="index2a62.html?q=node/3">Long &amp; Short Papers</a></li>
<li class="leaf"><a href="indexd46b.html?q=node/16">Posters</a></li>
<li class="leaf"><a href="indexb0c5.html?q=node/37">Lab/Project Presentations</a></li>
<li class="leaf"><a href="indexca97.html?q=node/36">Industrial Contributions</a></li>
<li class="leaf"><a href="index9f1c.html?q=node/18">Research Demos</a></li>
<li class="leaf"><a href="index0f22.html?q=node/4">Workshops</a></li>
<li class="leaf"><a href="index7c44.html?q=node/14">Tutorials</a></li>
<li class="leaf"><a href="index763f.html?q=node/15">Panels</a></li>
<li class="leaf"><a href="index06d0.html?q=node/20">Videos</a></li>
<li class="leaf"><a href="index28fc.html?q=node/17">Exhibitors and Supporters</a></li>
<li class="leaf"><a href="indexd917.html?q=node/19">Student Volunteers</a></li>
<li class="last leaf"><a href="index89f8.html?q=node/40">Doctoral Consortium</a></li>
</ul></li>
<li class="expanded"><a href="index72dc.html?q=node/23">Participate</a><ul class="menu"><li class="first leaf"><a href="index8860.html?q=node/7">Venue and transportation</a></li>
<li class="leaf"><a href="index71da.html?q=node/24" title="added Saintes-Maries-de-la-Mer and Aigues-Mortes in the &quot;must-see&quot; section">Arles-Camargue-Provence</a></li>
<li class="leaf"><a href="index77ab.html?q=node/22">Registration</a></li>
<li class="leaf"><a href="indexf7a0.html?q=node/41">Accomodations</a></li>
<li class="leaf"><a href="index3f6e.html?q=node/43">Visa FAQ</a></li>
<li class="last leaf"><a href="index5854.html?q=node/52">Presenter Instructions</a></li>
</ul></li>
<li class="expanded"><a href="index63ba.html?q=node/8">Committees</a><ul class="menu"><li class="first leaf"><a href="indexf02c.html?q=node/11">Organization Committee</a></li>
<li class="leaf"><a href="indexd022.html?q=node/12">Program Committee</a></li>
<li class="last leaf"><a href="index67db.html?q=node/13">Steering Committee</a></li>
</ul></li>
<li class="last leaf"><a href="http://3dui.org/" title="">IEEE 3DUI 2015</a></li>
</ul>      </div>
    </div>
    </div>
  <div id="content-container">

    
 <div id="page-container">
  
  
  <div id="content">
  <div class="breadcrumb"><h2 class="element-invisible">You are here</h2><nav class="breadcrumb"><a href="index.html">Home</a> » <a href="index5ffe.html?q=node/6">Program</a> » Research Demos</nav></div>  <section id="main" role="main" class="post">
        <a id="main-content"></a>
            <div class="title"><h2 class="title" id="page-title">Research Demos</h2></div>                <div class="region region-content">
  <div id="block-system-main" class="block block-system">

      
  <div class="content">
                            
      
    
  <div class="content">
    <div class="field field-name-body field-type-text-with-summary field-label-hidden"><div class="field-items"><div class="field-item even" property="content:encoded"><p><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">The following Research Demos have been accepted for publication at IEEE VR 2015:</span></span></p>
<ul><li><a href="index6f9b.html?q=node/49#115">Research Demo 1: </a><a href="#demo1">Non-Obscuring Binocular Eye Tracking for Wide Field-of-View Head-mounted-Display</a></li>
<li><a href="index6f9b.html?q=node/49#115">Research Demo </a><a href="#demo2">2: Dynamic 3D Interaction using an Optical See-through HMD</a></li>
<li><a href="#demo3">Research Demo 3: Laying out spaces with virtual reality</a></li>
<li><a href="#demo4">Reseach Demo 4: Three-dimensional VR Interaction Using the Movement of a Mobile Display</a></li>
<li><a href="#demo5">Research Demo 5: Towards a High Resolution Grip Measurement Device for Orthopaedics</a></li>
<li><a href="#demo6">Research Demo 6: Aughanded Virtuality - The Hands in the Virtual Environment</a></li>
<li><a href="#demo7">Research Demo 7: Never Blind VR</a></li>
<li><a href="#demo8">Research Demo 8: Magic Pot 360: Free Viewpoint shape Display</a></li>
<li><a href="#demo9">Research Demo 9: Various Forms of Tactile Feedback Displayed on the Back of the Tablet</a></li>
<li><a href="#demo10">Research Demo 10: Presentation of Virtual Liquid by Modeling Vibration of a Japanese Sake Bottle</a></li>
<li><a href="#demo11">Research Demo 11: Walking recording and experience system by Visual Psychophysics Lab</a></li>
<li><a href="#demo12">Research Demo 12: Live Streaming System for Omnidirectional Video</a></li>
<li><a href="#demo13">Research Demo 13: Blind in a Virtual World</a></li>
<li><a href="#demo14">Research Demo 14: Exploring Virtual Realityand Prosthetic Training</a></li>
<li><a href="#demo15">Research Demo 15: VR based Surgical Training</a></li>
<li><a href="#demo16">Research Demo 16: A Multi-Projector Display System of Arbitrary Shape, Size and Resolution</a></li>
<li><a href="#demo17">Research Demo 17: A 3D Heterogeneous Interactive Web Mapping Application</a></li>
<li><a href="#demo18">Research Demo 18: Virtual Reality toolbox for experimental psychology – Research Demo</a></li>
<li><a href="#demo19">Research Demo 19: Augmented Reality maintenance demonstrator</a></li>
<li><a href="#demo20">Research Demo 20: Augmented reality for maintenance application on a mobile platform</a></li>
<li><a href="#demo21">Research Demo 21: Underwater Integral Photography</a></li>
<li><a href="#demo22">Research Demo 22: Epilogi in Crisis</a></li>
</ul><p> </p>
<hr /><p class="rtecenter"><a id="demo8" name="demo8"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Research Demo 8</span></span></p>
<p class="rtecenter"><strong><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">MagicPot360: Free Viewpoint shape Display Modifying the Perception of shape</span></span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yuki Ban, Takuji Narumi, Tomohiro Tanikawa, and Michitaka Hirose</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Graduate School of Information Science and Technology, The University of Tokyo</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Abstract: In this paper we developed the free-viewpoint curved surface shape display, using the effect of visuo-haptic interaction. In our research, we aim to realize the simple mechanic visuo-haptic system with which we can touch various objects with our real hands. We proposed the free-viewpoint shape display system in which users can touch a virtual object with various shape through the tablet device, while walking around the object. The system figured the difference of shape between the real object and the virtual one from the current viewpoint, and calculates the amount of displacement of the touching hand image to fit to the virtual object, in real time. This modification of the movement of a hand image evokes the effect of the visuo-haptic interaction, and enable users to feel touching various shapes from various viewpoints, although actually users touch a physically static object.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/1_1.jpg" style="height:310px; width:453px" width="453" height="310" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo17" name="demo17"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 17</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">3D Heterogeneous Interactive Web Mapping Application</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Quoc-Dinh Nguyen, Alexandre Devaux, Mathieu Bredif and Nicolas Paparoditis</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Université Paris-Est, IGN, SRIG, MATIS</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">The internet browsers nowadays show incredible possibilities with HTML5. It makes it possible to use all the power of your device such as the GPU and all its sensors, GPS, accelerometer, camera, etc. The ability to put hardware-accelerated 3D content in the browser provides a way for the creation of new web based applications that were previously the exclusive domain of the desktop environment. This paper introduces a novel implementation of a 3D GIS WebGL-based navigation system which allows end-users to navigate in a 3D realistic and immersive urban scene, to interact with different spatial data such as panoramic image, laser, 3D-city model, and vector data with modern functionalities such as using your smartphone as a remote, render for 3D screen and make the scene dynamic.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/2_1.jpg" style="height:250px; width:441px" width="441" height="250" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo5" name="demo5"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 5</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Towards a High Resolution Grip Measurement Device for Orthopaedics</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Marc R. Edwards Peter Vangorp Nigel W. John</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Visualisation and Medical Graphics Laboratory, Bangor University, United Kingdom</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">We are developing a novel device for measuring hand power grip using frustrated total internal reflection of light in acrylic. Our method uses a force sensitive resistor to calibrate the force of a power grip as a function of the area and light intensity. This research is work in progress but results so far augur well for its applicability in medical and other application areas. The grip measurement device allows the patient and doctor to see the change in grip over time and projects this information directly onto the back of the patient's hand.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/resize/3_1-200x269.jpg" style="height:269px; width:200px" width="200" height="269" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo6" name="demo6"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Research Demo 6</span></span></p>
<p class="rtecenter"><strong><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Aughanded Virtuality - The Hands in the Virtual Environment</span></span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Tobias Günther Ingmar S. Franke Rainer Groh</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Technische Universität Dresden</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">The leading motive of the research demo is the utilization of the Augmented Virtuality technology and in particular the egocentric representation of real body parts in virtual and immersive environments. In this context, a prototypical application was developed, which superimposes the current view of the user’s hands on the virtual scene in real-time. This is achieved in form of a captured video stream. Advantages compared to virtual avatars arise from the detailed and individual representation of the user’s body and the saving of complex tracking hardware. Due to the integration of a toolbox, the visual appearance of the video overlay is highly customizable.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/4_1.jpg" style="height:267px; width:453px" width="453" height="267" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo19" name="demo19"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 19</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Augmented Reality maintenance demonstrator and associated modelling</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">V. Havard, D. Baudry, A. Louis, B. Mazari</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">LUSINE and IRISE laboratories, CESI</span></span></p>
<p class="rtecenter"> </p>
<p><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Augmented reality allows to add virtual object in real scene. It has an increasing interest last years since mobile device becomes performant and cheap. The augmented reality is used in different domains, like maintenance, training, education, entertainment or medicine. The demonstrator we show is focused on maintenance operations. A step by step process is presented to the operator in order to maintain an element of a system. Based on this demonstration, we will explain the modelling we propose allowing describing an entire maintenance process with augmented reality. Indeed it is still difficult creating augmented reality application without computer programming skills. The proposed model will allow to create an authoring tool - or to plug to an existing one - in order to create augmented reality process without deep computer programming skills.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/5_1.jpg" style="height:183px; width:414px" width="414" height="183" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo10" name="demo10"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 10</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Presentation of Virtual Liquid by Modeling Vibration of a Japanese Sake Bottle</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Sakiko Ikeno<sup>1</sup>, Ryuta Okazaki<sup>12</sup>, Taku Hachisu<sup>12</sup>, Hiroyuki Kajimoto<sup>13</sup></span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><sup>1</sup>The University of Electro-Communications, <sup>2</sup>JSPS Research Fellow, <sup>3</sup>Japan Science and Technology Agency</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">It is known that visual, auditory, and tactile modalities affect the experiences of eating and drinking. One such example is the “glug” sound and vibration from a Japanese sake bottle when pouring liquid. Our previous studies have modeled the wave of the vibration by summation of two decaying sinusoidal waves with different frequencies. In this paper, to enrich expression of various types of liquid, we included two new properties of liquid: the viscosity and the residual amount of liquid, both based on recorded data.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/6_0.jpg" style="height:301px; width:453px" width="453" height="301" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo11" name="demo11"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Research Demo 11</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Walking recording and experience system by Visual Psychophysics Lab</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Atsuhiro Fujita, Shohei Uedam, Junki Nozawa </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Graduate School of Engineering Toyohashi University of Technology</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Koichi Hirota </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Interfaculty Initiative in Information Studies The University of Tokyo</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Yasushi Ikei </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Faculty of System Design Tokyo Metropolitan University</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Michiteru Kitazaki</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Department of Computer Science and Engineering Toyohashi University of Technology</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: We aim to develop a virtual-reality system that records a person's walking experience and gives other users the experience of his/her walking. We recorded stereo motion images of video cameras on a person's forehead with synchronous acceleration data of ankles. Then, we presented stereo motion images on a HMD with synchronous vibrations on soles of observer's feet. Observers reported better experience of vection, walking, and tele-existence from stereo images with vibrations than without vibrations. We recorded walking experiences of different body sizes including a child (130 cm tall) and those of a dog. Observers can partly experience child's walking and even dog's running.</span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/7.jpg" style="height:256px; width:453px" width="453" height="256" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo9" name="demo9"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 9</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Various Forms of Tactile Feedback Displayed on the Back of the Tablet: Latency Minimized by Using Audio Signal to Control Actuators</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Itsuo Kumazawa*, Kyohei Sugiyama, Tsukasa Hayashi, Yasuhiro Takatori and Shunsuke Ono</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Imaging Science and Engineering Laboratory, Tokyo Institute of Technology</span></span></p>
<p class="rtecenter"> </p>
<p><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">The front face of the tablet style smartphone or computer is dominated by a touch screen. As a finger operation on the touch screen disturbs its visibility, it is assumed a finger touches the screen instantly. Under such restriction, use of the rear surface of the tablet for tactile display is promising as the fingers constantly touch the back face and feel the tactile information. In our presentation, various tactile feedback mechanisms implemented on the back face are demonstrated and the latency of the feedback and its effect on the usability are evaluated for different communication means to control actuators such as wireless LAN, Bluetooth and audio signals. It is shown that the audio signal is promising to generate quick tactile feedback.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/8.jpg" style="height:208px; width:268px" width="268" height="208" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo3" name="demo3"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Research Demo 3</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><strong>Laying out spaces with virtual reality</strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Morgan Le Chénéchal, Jérémy Lacoche, Cyndie Martin, Jérôme Royan</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">IRT</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">When dealing with real estate business, it is quite difficult for estate agents to make customers understand the potential and the volumes of free spaces. Thus, we propose an application that aims to solve these issues based on a laying out scenario in which a seller and a customer collaborate. As the roles of both users are different, we propose an asymmetric collaboration where the two users do not use the same interaction setup and do not benefit from the same interaction capabilities.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/9.jpg" style="height:341px; width:453px" width="453" height="341" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo16" name="demo16"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 16</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">A Multi-Projector Display System of Arbitrary Shape, Size and Resolution</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Aditi Majumder, Duy-Quoc Lai, Mahdi Abbaspour Tehrani</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">University of California, Irvine</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">In this demo we will demonstrate integration of general content delivery from a windows desktop to a multi-projector display of arbitrary, shape, size and resolution automatically calibrated using our calibration methods. We have developed these sophisticated completely automatic geometric and color registration techniques in our lab for deploying seamless multi-projector displays on popular non-planar surfaces (e.g. cylinders, domes, truncated domes). This work has gotten significant attention in both VR and Visualization venues in the past 5 years and this will be the first time such calibration will be integrated with content delivery.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/10.jpg" style="height:314px; width:453px" width="453" height="314" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo13" name="demo13"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 13</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Blind in a Virtual World: Mobility-Training Virtual Reality Games for Users who are Blind</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Shachar Maidenbaum,    Amir Amedi</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Hebrew University of Jerusalem, ELSC, IMRIC, Israel</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">One of the main challenges facing the practical utilization of new assistive technology for the blind is the process of training. This is true both for mastering the device and even more importantly for learning to use it in specific environments. Such training usually requires external help which is not always available, can be costly, and attempts to navigate without such preparation can be dangerous. Here we will demonstrate several games which were developed in our lab as part of the training programs for the EyeCane, which augments the traditional White-Cane with additional distance and angles. These games avoid the above-mentioned problems of availability, cost and safety and additionally utilize gamification techniques to boost the training process. Visitors to the demonstration will use these devices to perform simple in-game virtual tasks such as finding the exit from a room or avoiding obstacles while wearing blindfolds.</span></span></p>
<hr /><p class="rtecenter"><a id="demo21" name="demo21"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 21</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Underwater Integral Photography</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Nahomi Maki, Kazuhisa Yanaka</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Kanagawa Institute of Technology</span></span></p>
<p class="rtecenter"> </p>
<p><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">A novel integral photography (IP) system in which the amount of popping out is more than three times larger than usual is demonstrated in this study. If autostereoscopic display is introduced into virtual reality, IP is an ideal candidate because not only the horizontal but also the vertical parallax can be obtained. However, the amount of popping out obtained by IP is generally far less than that obtained by head-mounted display because the ray density decreases when the viewer is distant from the fly’s eye lens. Although a solution is to extend the focal length of the fly’s eye lens, this lens is difficult to manufacture. We address this problem by simply immersing the fly’s eye lens into water to extend the effective focal length.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/11.jpg" style="height:188px; width:311px" width="311" height="188" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo15" name="demo15"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 15</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Low Cost Virtual Reality for Medical Training</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Aman S. Mathur</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Jaipur Engineering College &amp; Research Centre</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">This demo depicts a low cost virtual reality set-up that may be used for medical training and instruction purposes. Using devices such as the Oculus Rift and Razer Hydra, an immersive experience, including hand interactivity can be given. Software running on a PC integrates these devices and presents an interactive and immersive training environment, where trainees are asked to perform a mixed bag of both, simple and complex tasks. These tasks range from identification of certain organs to performing of an actual incision. Trainees learn by doing, albeit in the virtual world. Components of the system are relatively affordable and simple to use, thereby making such a set-up incredibly easy to deploy.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/12.jpg" style="height:239px; width:316px" width="316" height="239" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo7" name="demo7"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 7</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">“Never Blind VR” Enhancing the Virtual Reality Headset Experience with Augmented Virtuality</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">David Nahon, Geoffrey Subileau, Benjamin Capel</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Dassault Systèmes, Passion for Innovation Institute, Immersive Virtuality (iV) Lab</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">In this demo, we share our findings in building real-time 3D experiences with consumer headsets so as to go beyond the first person shooter gaming usage for which they are designed. We address the key problems of such user experiences which are to isolate the user from his own body, have him lose contact with other people in the room and with the real world. To solve those issues we use an off-the-shelf Kinect for Windows v2 to inject some reality in the virtuality.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/13.jpg" style="height:329px; width:441px" width="441" height="329" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo12" name="demo12"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 12</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Live Streaming System for Omnidirectional Video</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Daisuke Ochi, Akio Kameda, Yutaka Kunita Akira Kojima</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">NTT Media Intelligence Laboratories</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Shinnosuke Iwaki</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">DWANGO Co., Ltd.</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">NTT Media Intelligence Laboratories and DWANGO Co., Ltd. have jointly developed a virtual reality system that enables users to have an immersive experience visiting a remote site. This system makes it possible for users to watch video content wherever they want to watch it by using interactive streaming technology that selectively streams the user’s watching section at a high bitrate in a limited network bandwidth. Applying this technology to omnidirectional video allows users to experience feelings of presence through the use of an intuitive head mount display. The system has also been released on a commercial platform and successfully streamed a real-time event. A demonstration is planned in which the details of the system and the streaming service results obtained with it will be presented.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/14.jpg" style="height:332px; width:453px" width="453" height="332" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo22" name="demo22"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 22</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Επιλογή* in Crisis**</span></strong></p>
<p class="rtecenter"><strong><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">*επιλογή = επί (= on, for) + λόγος (= reason, cause, speech) επιλογή (= choice) | **crisis = κρίση (= a time of intense difficulty, judgment)</span></span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Prof. Manthos Santorineos, Dr. Stavroula Zoi, Dr. Nefeli Dimitriadi, Taxiarchis Diamantopoulos, John Bardakos, Christina Chrysanthopoulou, Ifigeneia Mavridou, Anna Meli, Nikos Papadopoulos, Argyro Papathanasiou, Maria Velaora</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Greek-French Master «Art, virtual reality and multiuser systems of artistic expression» Athens School of Fine Arts</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">“Επιλογή in Crisis” is a work in progress that has been developed by the research group of the Greek-French Master entitled "Art, virtual reality and multiuser systems of artistic expression", in a collaboration between the Athens School of Fine Arts and the University Paris8 Saint-Denis. It concerns an interactive project which is in-between a research tool and experimental game, that takes place in a virtual reality environment. It aims to immerse the player inside a system in crisis, so that he is not a mere spectator but feels that he shares responsibility for the crisis and has to act to resolve it. The actions of the player are measured and “judged” (by the game mechanism itself), thus determining the stability or instability of the system.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/15.jpg" style="height:212px; width:453px" width="453" height="212" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo14" name="demo14"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 14</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Exploring Virtual Reality and Prosthetic Training</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Mr Ivan Phelan</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Culture, Communication and Computing Research Institute Sheffield Hallam University Sheffield, United Kingdom</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Dr Madelynne Arden</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Faculty of Development and Society Sheffield Hallam University Sheffield, United Kingdom</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Mrs Carol Garcia </span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Faculty of Health and Wellbeing Sheffield Hallam University Sheffield, United Kingdom</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Dr. Chris Roast</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Culture, Communication and Computing Research Institute Sheffield Hallam University Sheffield, United Kingdom</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Working together with health care professionals and a world leading bionic prosthetic maker we created a prototype that aims to decrease the time it takes for a transradial amputee to train how to use a Myoelectric prosthetic arm. Our research indicates that the Oculus Rift, Microsoft’s Kinect and the Thalmic Labs Myo gesture control armband will allow us to create a unique, cost effective training tool that could be beneficial to amputee patients.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/16.jpg" style="height:388px; width:350px" width="350" height="388" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo20" name="demo20"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Research Demo 20</span></span></p>
<p class="rtecenter"><strong><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Augmented reality for maintenance application on a mobile platform</span></span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><sup>1</sup>Lakshmprabha N.S., <sup>2</sup>Panagiotis Mousouliotis, <sup>3</sup>Loukas Petrou, <sup>2</sup>Stathis Kasderidisk, <sup>1</sup>Olga Beltramello</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><sup>1</sup>European Organization for Nuclear Research, CERN</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><sup>2</sup>NOVOCAPTIS</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><sup>3</sup>Aristotle University of Thessaloniki</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Pose estimation is a major requirement for any augmented reality (AR) application. Cameras and inertial measurement units (IMUs) have been used for pose estimation not only in AR but also in many other fields. The level of accuracy and pose update required in an AR application is more demanding than in any other field. In certain AR applications, (maintenance for example) a small change in pose can cause a huge deviation in the rendering of the virtual content. This misleads the user in terms of an object location and can display incorrect information. Further, the huge amount of processing power required for the camera based pose estimation results in a bulky system. This reduces the mobility and ergonomics of the system. This demonstration shows a fast pose estimation using a camera and an IMU on a mobile platform for augmented reality in a maintenance application.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/17.jpg" style="height:322px; width:374px" width="374" height="322" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo1" name="demo1"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 1</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Non-Obscuring Binocular Eye Tracking for Wide Field-of-View Head-mounted-Displays</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Michael Stengel*, Steve Grogorick*, Martin Eisemann*, Elmar Eisemann+, Marcus Magnor+</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">*TU Braunschweig, +TU Delft</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">We present a complete hardware and software solution for integrating binocular eye tracking into current state-of-the-art lens-based Head-mounted Displays (HMDs) without affecting the user’s wide field-of-view off the display. The system uses robust and efficient new algorithms for calibration and pupil tracking and allows realtime eye tracking and gaze estimation. Estimating the relative gaze direction of the user opens the door to a much wider spectrum of virtual reality applications and games when using HMDs. We show a 3d-printed prototype of a low-cost HMD with eye tracking that is simple to fabricate and discuss a variety of VR applications utilizing gaze estimation.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/18.jpg" style="height:303px; width:453px" width="453" height="303" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo2" name="demo2"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 2</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Dynamic 3D Interaction using an Optical See-through HMD</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Nozomi Sugiura, Takashi Komuro</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Saitama University</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">We propose a system that enables dynamic 3D interaction with real and virtual objects using an optical see-through head-mounted display and an RGB-D camera. The virtual objects move according to physical laws. The system uses a physics engine for calculation of the motion of virtual objects and collision detection. In addition, the system performs collision detection between virtual objects and real objects in the three-dimensional scene obtained from the camera which is dynamically updated. A user wears the device and interacts with virtual objects in a seated position. The system gives users a great sense of reality through an interaction with virtual objects.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/19.jpg" style="height:266px; width:453px" width="453" height="266" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo18" name="demo18"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 18</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Virtual Reality toolbox for experimental psychology – Research Demo</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Madis Vasser*, Markus Kängsepp, Kälver Kilvits, Taavi Kivisik, Jaan Aru</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">University of Tartu</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">We present a general toolbox for virtual reality (VR) research in the field of psychology. Our aim is to simplify the generation and setup of complicated VR scenes for researchers. Various study protocols about perception, attention, cognition and memory can be constructed using our toolbox. Here we specifically showcase a fully functional demo for change blindness phenomena.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/20.jpg" style="height:276px; width:453px" width="453" height="276" /></span></span></p>
<hr /><p class="rtecenter"><a id="demo4" name="demo4"></a></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Research Demo 4</span></p>
<p class="rtecenter"><strong><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Three-dimensional VR Interaction Using the Movement of a Mobile Display</span></strong></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Lili Wang, Takashi Komuro</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">Saitama University</span></span></p>
<p class="rtecenter"> </p>
<p class="rtejustify"><span style="font-family:arial,helvetica,sans-serif; font-size:14px">Abstract: </span><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif">In this study, we propose a VR system for allowing various types of interaction with virtual objects using an autostereoscopic mobile display and an accelerometer. The system obtains the orientation and motion information from the accelerometer attached to the mobile display and reflects them to the motion of virtual objects. It can present 3D images with motion parallax by estimating the position of the user’s viewpoint and by displaying properly projected images. Furthermore, our method enables to connect the real space and the virtual space seamlessly through the mobile display by determining the coordinate system so that one of the horizontal surfaces in the virtual space coincides with the display surface. To show the effectiveness of this concept, we implemented an application to simulate food cooking by regarding the mobile display as a frying pan.</span></span></p>
<p class="rtecenter"><span style="font-size:14px"><span style="font-family:arial,helvetica,sans-serif"><img alt="" src="sites/default/files/21.jpg" style="height:318px; width:453px" width="453" height="318" /></span></span></p>
<hr /><p> </p>
</div></div></div>  </div>

      <footer>
                </footer>
  
    </div>
  
</div> <!-- /.block -->
</div>
  </section> <!-- /#main -->
  </div>

      <aside id="sidebar-second" role="complementary" class="sidebar clearfix">
     <div class="region region-sidebar-second">
  <div id="block-block-6" class="block block-block">

      
  <div class="content">
    <p class="rtecenter"><span style="font-family:lucida sans unicode,lucida grande,sans-serif"><span style="font-size:18px"><strong><a href="indexe737.html?q=node/51#overlay-context=node/51%3Fq%3Dnode/51" target="_blank">Exhibitors and Supporters</a></strong></span></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><strong><span style="font-size:16px"><span style="font-family:arial,helvetica,sans-serif">Platinum Level</span></span></strong></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/INRIA_SCIENTIFIQUE_UK_CMJN-180x65.png" style="height:65px; line-height:20.7999992370605px; text-align:center; width:180px" width="180" height="65" /></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/nsf-180x180.jpg" style="height:180px; width:180px" width="180" height="180" /></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:16px"><span style="font-family:arial,helvetica,sans-serif"><strong>Silver Level</strong></span></span></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/AFRV_small-180x63.png" style="height:63px; width:180px" width="180" height="63" /></p>
<p class="rtecenter"><span style="font-size:16px"><img alt="" src="sites/default/files/resize/Logo-AGP_carr%c3%a9-180x180.jpg" style="height:180px; width:180px" width="180" height="180" /></span></p>
<p class="rtecenter"><span style="font-size:16px"><span style="font-family:arial,sans-serif"><img alt="" src="sites/default/files/resize/ART-logo%20-%20black-180x68.png" style="height:68px; width:180px" width="180" height="68" /></span></span></p>
<p class="rtecenter"><span style="font-size:16px"><img alt="" src="sites/default/files/resize/Blue%20Vicon%20Logo-180x42.jpg" style="height:42px; width:180px" width="180" height="42" /></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-family:arial,helvetica,sans-serif"><span style="font-size:16px"><strong>Bronze Level</strong></span></span></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/3DRudd_logo_Coul-180x65.png" style="height:65px; width:180px" width="180" height="65" /></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/logo_GENESIS%20Haute%20D%c3%a9finition-180x42.jpg" style="height:42px; line-height:1.6; width:180px" width="180" height="42" /></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/haption_1-180x121.jpg" style="font-size:16px; height:121px; line-height:1.6; width:180px" width="180" height="121" /></p>
<p class="rtecenter"><span style="font-size:16px"><img alt="" src="sites/default/files/resize/MiddleVR-180x127.png" style="height:127px; width:180px" width="180" height="127" /></span></p>
<p class="rtecenter"><span style="font-size:16px"><img alt="" src="sites/default/files/resize/okta-180x74.png" style="height:74px; width:180px" width="180" height="74" /></span></p>
<p class="rtecenter"><span style="font-size:16px"><span style="font-family:calibri,sans-serif"><img alt="" src="sites/default/files/resize/razer-sensics-180x50.png" style="height:50px; width:180px" width="180" height="50" /></span></span></p>
<p class="rtecenter"><span style="font-size:16px"><img alt="" src="sites/default/files/resize/Logo_Technicolor_Q-180x69.png" style="height:69px; width:180px" width="180" height="69" /></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:16px"><span style="font-family:arial,helvetica,sans-serif"><strong>Publishers</strong></span></span></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/PRES_full_logo_gray-180x63.gif" style="height:63px; width:180px" width="180" height="63" /></p>
<p class="rtecenter"><span style="font-size:16px"><strong><img alt="" src="sites/default/files/resize/Springer_cmyk-180x48.png" style="height:48px; width:180px" width="180" height="48" /></strong></span></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:16px"><span style="font-family:arial,helvetica,sans-serif"><strong>Supporters</strong></span></span></p>
<p class="rtecenter"><span style="font-size:16px"><span style="font-family:arial,helvetica,sans-serif"><strong><img alt="" src="sites/default/files/resize/disney-180x59.png" style="height:59px; width:180px" width="180" height="59" /></strong></span></span></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/egbanner_0-180x31.gif" style="height:31px; width:180px" width="180" height="31" /></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/egfr-logo_0-180x31.jpg" style="height:31px; width:180px" width="180" height="31" /></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/Persyval-lab_0-180x44.jpg" style="height:44px; width:180px" width="180" height="44" /></p>
<p class="rtecenter"> </p>
<p class="rtecenter"><span style="font-size:16px"><span style="font-family:arial,helvetica,sans-serif"><strong>Local Supporters</strong></span></span></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/logocg13%20-%20-180x77.jpg" style="height:77px; width:180px" width="180" height="77" /></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/logo_culturespace-180x32.jpg" style="height:32px; width:180px" width="180" height="32" /></p>
<p class="rtecenter"><img alt="" src="sites/default/files/resize/Ibis%20style-180x181.png" style="height:181px; width:180px" width="180" height="181" /></p>
<p class="rtecenter"> </p>
<p class="rtecenter"> </p>
<p class="rtecenter"> </p>
  </div>
  
</div> <!-- /.block -->
</div>
    </aside>  <!-- /#sidebar-second -->
   </div>
</div> 


   

       <div id="footer">
      <div class="footer_wrapper">
                     <div class="copyright">Copyright IEEE VR 2015 </div>
                </div>
  </div>
     </div>
</div>
  <script type="text/javascript" src="modules/statistics/statistics6d18.js?njikyt"></script>
</body>

<!-- Mirrored from www.ieeevr.org/2015/?q=node/33 by HTTrack Website Copier/3.x [XR&CO'2013], Tue, 24 Mar 2015 14:09:52 GMT -->
</html>
